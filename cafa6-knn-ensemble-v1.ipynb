{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"CONFIG = {\n    \"n_terms\": 5000,\n    \"knn_k\": 10,\n    \"knn_weight\": 0.4,\n    \"model_weight\": 0.6,\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"train_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\",\n        \"test_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T00:16:42.623944Z","iopub.execute_input":"2025-12-05T00:16:42.624205Z","iopub.status.idle":"2025-12-05T00:16:42.651952Z","shell.execute_reply.started":"2025-12-05T00:16:42.624184Z","shell.execute_reply":"2025-12-05T00:16:42.651099Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# === SENIOR SCIENTIST PIPELINE: TARGET 0.30+ ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import normalize\nimport gc\n\n# ============ CONFIG ============\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nCONFIG = {\n    \"n_terms\": 5000,  # INCREASED from 1500\n    \"knn_k\": 10,       # Number of neighbors\n    \"knn_weight\": 0.4, # Blend weight for KNN\n    \"model_weight\": 0.6,\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"train_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\",\n        \"test_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# ============ 1. LOAD DATA ============\nprint(\"\\n=== 1. Loading Data ===\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nprint(f\"Train: {train_emb.shape}, Test: {test_emb.shape}\")\n\n# Normalize (for cosine similarity in KNN)\nprint(\"Normalizing embeddings...\")\ntrain_emb_norm = normalize(train_emb, axis=1)\ntest_emb_norm = normalize(test_emb, axis=1)\n\n# Standard scale for neural network\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb_scaled = (train_emb - mean) / std\ntest_emb_scaled = (test_emb - mean) / std\n\n# Load Terms\nprint(\"Loading terms...\")\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# ============ 2. KNN INDEX ============\nprint(\"\\n=== 2. Building KNN Index ===\")\nknn_model = NearestNeighbors(n_neighbors=CONFIG[\"knn_k\"], metric='cosine', algorithm='brute', n_jobs=-1)\nknn_model.fit(train_emb_norm)\nprint(\"KNN Index Ready.\")\n\nprint(\"Finding neighbors for test set...\")\ndistances, indices = knn_model.kneighbors(test_emb_norm)\nsimilarities = 1 - distances\nprint(f\"KNN search complete. Shape: {indices.shape}\")\n\n# ============ 3. TAXONOMY ============\nprint(\"\\n=== 3. Preparing Taxonomy ===\")\ntrain_tax_df = pd.read_csv(CONFIG[\"paths\"][\"train_tax\"], sep=\"\\t\", header=None, names=[\"id\", \"tax_id\"])\ntest_tax_df = pd.read_csv(CONFIG[\"paths\"][\"test_tax\"], sep=\"\\t\", header=None, names=[\"id\", \"tax_id\"])\n\ntrain_tax_df['tax_id'] = train_tax_df['tax_id'].astype(str)\ntest_tax_df['tax_id'] = test_tax_df['tax_id'].astype(str)\n\nall_taxons = set(train_tax_df['tax_id'].unique()) | set(test_tax_df['tax_id'].unique())\ntax_list = sorted(list(all_taxons))\ntax_map = {t: i for i, t in enumerate(tax_list)}\nnum_taxons = len(tax_list)\n\ndef get_tax_indices(id_list, df):\n    mapping = dict(zip(df[\"id\"], df[\"tax_id\"]))\n    return np.array([tax_map.get(mapping.get(pid, \"0\"), 0) for pid in id_list], dtype=np.int32)\n\ntrain_tax_idx = get_tax_indices(train_ids, train_tax_df)\ntest_tax_idx = get_tax_indices(test_ids, test_tax_df)\nprint(f\"Taxonomy Ready. {num_taxons} species.\")\n\n# ============ 4. MODEL DEFINITION ============\nclass DeepTaxModel(nn.Module):\n    def __init__(self, n_feat, n_taxons, n_class):\n        super().__init__()\n        self.tax_emb = nn.Embedding(n_taxons, 128)\n        \n        self.net = nn.Sequential(\n            nn.Linear(n_feat + 128, 2048),\n            nn.BatchNorm1d(2048),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(2048, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, n_class)\n        )\n        \n    def forward(self, x, t):\n        t_vec = self.tax_emb(t)\n        combined = torch.cat([x, t_vec], dim=1)\n        return self.net(combined)\n\nclass MultiInputData(Dataset):\n    def __init__(self, X, T, y=None):\n        self.X = torch.from_numpy(X)\n        self.T = torch.from_numpy(T).long()\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i):\n        return (self.X[i], self.T[i], self.y[i]) if self.y is not None else (self.X[i], self.T[i])\n\n# ============ 5. TRAINING FUNCTION ============\ndef train_aspect(aspect_char, aspect_name):\n    print(f\"\\n>>> Training for {aspect_name} ({aspect_char})...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    num_classes = len(top_terms)\n    print(f\"Using {num_classes} GO terms\")\n    \n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Labels\"):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    ds = MultiInputData(train_emb_scaled, train_tax_idx, label_matrix)\n    loader = DataLoader(ds, batch_size=512, shuffle=True, num_workers=2)\n    \n    model = DeepTaxModel(1280, num_taxons, num_classes).to(device)\n    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=15)\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(15):\n        model.train()\n        total_loss = 0\n        for x, t, y in tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False):\n            x, t, y = x.to(device), t.to(device), y.to(device)\n            opt.zero_grad()\n            loss = loss_fn(model(x, t), y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n            total_loss += loss.item()\n        scheduler.step()\n        print(f\"Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n    \n    torch.save(model.state_dict(), f\"model_senior_{aspect_char}.pth\")\n    return top_terms, term_map, label_matrix\n\n# ============ 6. EXECUTE TRAINING ============\nresults = {}\nfor aspect_char, aspect_name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    top_terms, term_map, label_matrix = train_aspect(aspect_char, aspect_name)\n    results[aspect_char] = {\n        'terms': top_terms,\n        'term_map': term_map,\n        'labels': label_matrix\n    }\n\nprint(\"\\n✅ All Models Trained!\")\n\n# ============ 7. PREDICTION WITH KNN BLEND ============\nprint(\"\\n=== 7. Generating Blended Predictions ===\")\n\nwith open(\"submission_senior.tsv\", \"w\") as f:\n    \n    for aspect_char in ['F', 'P', 'C']:\n        print(f\"\\nPredicting {aspect_char}...\")\n        terms = results[aspect_char]['terms']\n        term_map = results[aspect_char]['term_map']\n        label_matrix = results[aspect_char]['labels']\n        num_classes = len(terms)\n        \n        model = DeepTaxModel(1280, num_taxons, num_classes).to(device)\n        model.load_state_dict(torch.load(f\"model_senior_{aspect_char}.pth\"))\n        model.eval()\n        \n        ds = MultiInputData(test_emb_scaled, test_tax_idx)\n        loader = DataLoader(ds, batch_size=1024)\n        \n        nn_preds = []\n        with torch.no_grad():\n            for x, t in tqdm(loader, desc=\"NN Pred\"):\n                x, t = x.to(device), t.to(device)\n                probs = torch.sigmoid(model(x, t)).cpu().numpy()\n                nn_preds.append(probs)\n        nn_preds = np.vstack(nn_preds)\n        \n        print(\"Computing KNN predictions...\")\n        knn_preds = np.zeros((len(test_ids), num_classes), dtype=np.float32)\n        \n        for i in tqdm(range(len(test_ids)), desc=\"KNN Pred\"):\n            neighbor_indices = indices[i]\n            neighbor_sims = similarities[i]\n            \n            for j, (n_idx, sim) in enumerate(zip(neighbor_indices, neighbor_sims)):\n                knn_preds[i] += sim * label_matrix[n_idx]\n            \n            knn_preds[i] /= (neighbor_sims.sum() + 1e-8)\n        \n        # BLEND\n        final_preds = (CONFIG[\"model_weight\"] * nn_preds) + (CONFIG[\"knn_weight\"] * knn_preds)\n        \n        print(f\"Writing {aspect_char} predictions...\")\n        for i, pid in enumerate(tqdm(test_ids)):\n            row = final_preds[i]\n            top_idx = np.argpartition(row, -100)[-100:]\n            \n            for idx in top_idx:\n                score = row[idx]\n                if score > 0.005:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{score:.4f}\\n\")\n        \n        del model, nn_preds, knn_preds, final_preds\n        torch.cuda.empty_cache()\n        gc.collect()\n\nprint(\"\\n✅ submission_senior.tsv created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T00:17:49.633805Z","iopub.execute_input":"2025-12-05T00:17:49.634081Z","iopub.status.idle":"2025-12-05T00:37:50.193694Z","shell.execute_reply.started":"2025-12-05T00:17:49.634051Z","shell.execute_reply":"2025-12-05T00:37:50.192600Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\n\n=== 1. Loading Data ===\nTrain: (82404, 1280), Test: (224309, 1280)\nNormalizing embeddings...\nLoading terms...\n\n=== 2. Building KNN Index ===\nKNN Index Ready.\nFinding neighbors for test set...\nKNN search complete. Shape: (224309, 10)\n\n=== 3. Preparing Taxonomy ===\nTaxonomy Ready. 9835 species.\n\n>>> Training for Function (F)...\nUsing 5000 GO terms\n","output_type":"stream"},{"name":"stderr","text":"Labels: 100%|██████████| 126836/126836 [00:04<00:00, 28658.09it/s]\n                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0199\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0020\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0019\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0018\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0017\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0016\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0016\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0015\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0015\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0014\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 0.0013\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 0.0013\n\n>>> Training for Process (P)...\nUsing 5000 GO terms\n","output_type":"stream"},{"name":"stderr","text":"Labels: 100%|██████████| 208567/208567 [00:07<00:00, 29131.93it/s]\n                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0226\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0043\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0039\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0037\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0036\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0035\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0034\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 0.0031\n\n>>> Training for Component (C)...\nUsing 2651 GO terms\n","output_type":"stream"},{"name":"stderr","text":"Labels: 100%|██████████| 157770/157770 [00:05<00:00, 28843.96it/s]\n                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0214\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0037\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0029\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0028\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0027\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0027\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0026\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0026\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0025\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0025\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 0.0025\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 0.0025\n\n✅ All Models Trained!\n\n=== 7. Generating Blended Predictions ===\n\nPredicting F...\n","output_type":"stream"},{"name":"stderr","text":"NN Pred: 100%|██████████| 220/220 [00:03<00:00, 62.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Computing KNN predictions...\n","output_type":"stream"},{"name":"stderr","text":"KNN Pred: 100%|██████████| 224309/224309 [00:11<00:00, 20117.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing F predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:11<00:00, 18999.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nPredicting P...\n","output_type":"stream"},{"name":"stderr","text":"NN Pred: 100%|██████████| 220/220 [00:03<00:00, 58.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Computing KNN predictions...\n","output_type":"stream"},{"name":"stderr","text":"KNN Pred: 100%|██████████| 224309/224309 [00:10<00:00, 20484.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing P predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:12<00:00, 18470.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nPredicting C...\n","output_type":"stream"},{"name":"stderr","text":"NN Pred: 100%|██████████| 220/220 [00:03<00:00, 66.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Computing KNN predictions...\n","output_type":"stream"},{"name":"stderr","text":"KNN Pred: 100%|██████████| 224309/224309 [00:09<00:00, 23200.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing C predictions...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:11<00:00, 19201.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ submission_senior.tsv created!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- GRAPH PROPAGATION ---\n!pip install obonet networkx -q\n\nimport obonet\nfrom tqdm import tqdm\n\nprint(\"Loading Graph...\")\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {}\nfor node in graph.nodes():\n    parents = [p for p in graph.successors(node) if p in graph.nodes()]\n    if parents: parent_map[node] = parents\n\nprint(\"Loading predictions...\")\nsubmission_dict = {}\nwith open(\"submission_senior.tsv\", \"r\") as f:\n    for line in tqdm(f):\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 3:\n            pid, term, score = parts\n            score = float(score)\n            if pid not in submission_dict: submission_dict[pid] = {}\n            submission_dict[pid][term] = score\n\nprint(\"Propagating...\")\nwith open(\"submission_final.tsv\", \"w\") as f_out:\n    for pid, preds in tqdm(submission_dict.items()):\n        final_scores = preds.copy()\n        queue = list(preds.keys())\n        visited = set(queue)\n        \n        while queue:\n            term = queue.pop(0)\n            score = final_scores.get(term, 0.0)\n            if term in parent_map:\n                for parent in parent_map[term]:\n                    old = final_scores.get(parent, 0.0)\n                    if score > old:\n                        final_scores[parent] = score\n                        if parent not in visited:\n                            queue.append(parent)\n                            visited.add(parent)\n        \n        sorted_terms = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:100]\n        for term, score in sorted_terms:\n            if score > 0.001:\n                f_out.write(f\"{pid}\\t{term}\\t{score:.4f}\\n\")\n\nprint(\"✅ submission_final.tsv ready! Rename to submission.tsv and submit.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T00:39:05.829488Z","iopub.execute_input":"2025-12-05T00:39:05.829918Z","iopub.status.idle":"2025-12-05T00:40:12.255955Z","shell.execute_reply.started":"2025-12-05T00:39:05.829899Z","shell.execute_reply":"2025-12-05T00:40:12.254975Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nLoading Graph...\nLoading predictions...\n","output_type":"stream"},{"name":"stderr","text":"10310571it [00:09, 1071658.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Propagating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:48<00:00, 4604.41it/s]","output_type":"stream"},{"name":"stdout","text":"✅ submission_final.tsv ready! Rename to submission.tsv and submit.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === OPTIMIZED PIPELINE (NO KNN) ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nCONFIG = {\n    \"n_terms\": 3000,  # Sweet spot between coverage and sparsity\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"train_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\",\n        \"test_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# ============ LOAD DATA ============\nprint(\"Loading data...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\n# Standard scale\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# Taxonomy\ntrain_tax_df = pd.read_csv(CONFIG[\"paths\"][\"train_tax\"], sep=\"\\t\", header=None, names=[\"id\", \"tax_id\"])\ntest_tax_df = pd.read_csv(CONFIG[\"paths\"][\"test_tax\"], sep=\"\\t\", header=None, names=[\"id\", \"tax_id\"])\ntrain_tax_df['tax_id'] = train_tax_df['tax_id'].astype(str)\ntest_tax_df['tax_id'] = test_tax_df['tax_id'].astype(str)\n\nall_taxons = set(train_tax_df['tax_id'].unique()) | set(test_tax_df['tax_id'].unique())\ntax_map = {t: i for i, t in enumerate(sorted(all_taxons))}\nnum_taxons = len(tax_map)\n\ndef get_tax_indices(id_list, df):\n    mapping = dict(zip(df[\"id\"], df[\"tax_id\"]))\n    return np.array([tax_map.get(mapping.get(pid, \"0\"), 0) for pid in id_list], dtype=np.int32)\n\ntrain_tax_idx = get_tax_indices(train_ids, train_tax_df)\ntest_tax_idx = get_tax_indices(test_ids, test_tax_df)\n\n# ============ OPTIMIZED MODEL ============\nclass OptimizedModel(nn.Module):\n    def __init__(self, n_feat, n_taxons, n_class):\n        super().__init__()\n        self.tax_emb = nn.Embedding(n_taxons, 64)  # Smaller tax embedding\n        \n        self.net = nn.Sequential(\n            nn.Linear(n_feat + 64, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),  # Higher dropout for generalization\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, n_class)\n        )\n        \n    def forward(self, x, t):\n        t_vec = self.tax_emb(t)\n        return self.net(torch.cat([x, t_vec], dim=1))\n\nclass MultiInputData(Dataset):\n    def __init__(self, X, T, y=None):\n        self.X = torch.from_numpy(X)\n        self.T = torch.from_numpy(T).long()\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i):\n        return (self.X[i], self.T[i], self.y[i]) if self.y is not None else (self.X[i], self.T[i])\n\n# ============ TRAINING ============\ndef train_aspect(aspect_char, aspect_name):\n    print(f\"\\n>>> Training {aspect_name}...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    num_classes = len(top_terms)\n    print(f\"Terms: {num_classes}\")\n    \n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], {t: i for i, t in enumerate(top_terms)}[row['term']]] = 1.0\n    \n    ds = MultiInputData(train_emb, train_tax_idx, label_matrix)\n    loader = DataLoader(ds, batch_size=256, shuffle=True)\n    \n    model = OptimizedModel(1280, num_taxons, num_classes).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)  # Higher weight decay\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(12):\n        model.train()\n        total = 0\n        for x, t, y in tqdm(loader, leave=False):\n            x, t, y = x.to(device), t.to(device), y.to(device)\n            opt.zero_grad()\n            loss = loss_fn(model(x, t), y)\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        print(f\"Epoch {epoch+1}: {total/len(loader):.4f}\")\n    \n    torch.save(model.state_dict(), f\"model_opt_{aspect_char}.pth\")\n    return top_terms\n\n# Train\nresults = {}\nfor char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    results[char] = train_aspect(char, name)\n\n# ============ PREDICT ============\nprint(\"\\n>>> Predicting...\")\nwith open(\"submission_optimized.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        terms = results[char]\n        model = OptimizedModel(1280, num_taxons, len(terms)).to(device)\n        model.load_state_dict(torch.load(f\"model_opt_{char}.pth\"))\n        model.eval()\n        \n        ds = MultiInputData(test_emb, test_tax_idx)\n        loader = DataLoader(ds, batch_size=1024)\n        \n        preds = []\n        with torch.no_grad():\n            for x, t in tqdm(loader):\n                x, t = x.to(device), t.to(device)\n                preds.append(torch.sigmoid(model(x, t)).cpu().numpy())\n        \n        all_preds = np.vstack(preds)\n        \n        for i, pid in enumerate(test_ids):\n            top_idx = np.argpartition(all_preds[i], -80)[-80:]\n            for idx in top_idx:\n                if all_preds[i, idx] > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{all_preds[i, idx]:.4f}\\n\")\n        \n        del model, preds\n        gc.collect()\n\nprint(\"Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T00:54:53.941117Z","iopub.execute_input":"2025-12-05T00:54:53.941588Z","iopub.status.idle":"2025-12-05T01:01:54.326814Z","shell.execute_reply.started":"2025-12-05T00:54:53.941567Z","shell.execute_reply":"2025-12-05T01:01:54.325891Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\nLoading data...\n\n>>> Training Function...\nTerms: 3000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 121614/121614 [00:32<00:00, 3727.86it/s]\n                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: 0.0176\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: 0.0067\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12: 0.0070\n\n>>> Training Process...\nTerms: 3000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 181213/181213 [00:47<00:00, 3789.84it/s]\n                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: 0.0202\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: 0.0092\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: 0.0092\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: 0.0093\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: 0.0093\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: 0.0093\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11: 0.0094\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12: 0.0094\n\n>>> Training Component...\nTerms: 2651\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157770/157770 [00:36<00:00, 4293.32it/s]\n                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: 0.0182\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: 0.0077\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: 0.0078\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: 0.0078\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12: 0.0079\n\n>>> Predicting...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:02<00:00, 86.57it/s]\n100%|██████████| 220/220 [00:02<00:00, 97.76it/s] \n100%|██████████| 220/220 [00:02<00:00, 97.10it/s] \n","output_type":"stream"},{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# === GRAPH PROPAGATION ===\n!pip install obonet networkx -q\n\nimport obonet\nfrom tqdm import tqdm\n\nprint(\"Loading Graph...\")\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {}\nfor node in graph.nodes():\n    parents = [p for p in graph.successors(node) if p in graph.nodes()]\n    if parents:\n        parent_map[node] = parents\n\nprint(\"Loading predictions...\")\nsubmission_dict = {}\nwith open(\"submission_optimized.tsv\") as f:\n    for line in tqdm(f):\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 3:\n            pid, term, score = parts\n            score = float(score)\n            if pid not in submission_dict:\n                submission_dict[pid] = {}\n            submission_dict[pid][term] = score\n\nprint(\"Propagating...\")\nwith open(\"submission_v2_final.tsv\", \"w\") as f_out:\n    for pid, preds in tqdm(submission_dict.items()):\n        final_scores = preds.copy()\n        queue = list(preds.keys())\n        visited = set(queue)\n        \n        while queue:\n            term = queue.pop(0)\n            score = final_scores.get(term, 0.0)\n            if term in parent_map:\n                for parent in parent_map[term]:\n                    old = final_scores.get(parent, 0.0)\n                    if score > old:\n                        final_scores[parent] = score\n                        if parent not in visited:\n                            queue.append(parent)\n                            visited.add(parent)\n        \n        sorted_terms = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:100]\n        for term, score in sorted_terms:\n            if score > 0.001:\n                f_out.write(f\"{pid}\\t{term}\\t{score:.4f}\\n\")\n\nprint(\"\\n submission_v2_final.tsv ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T01:02:32.601583Z","iopub.execute_input":"2025-12-05T01:02:32.601881Z","iopub.status.idle":"2025-12-05T01:04:07.921866Z","shell.execute_reply.started":"2025-12-05T01:02:32.601864Z","shell.execute_reply":"2025-12-05T01:04:07.920936Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nLoading Graph...\nLoading predictions...\n","output_type":"stream"},{"name":"stderr","text":"17361793it [00:15, 1097479.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Propagating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [01:13<00:00, 3058.19it/s]","output_type":"stream"},{"name":"stdout","text":"\n submission_v2_final.tsv ready!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === RECREATE 0.188 BASELINE ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCONFIG = {\n    \"n_terms\": 1500,  # BACK TO WHAT WORKED\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# LOAD DATA\nprint(\"Loading...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# SIMPLE MODEL (What got you 0.188)\nclass SimpleModel(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleData(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n\n# TRAIN\ndef train_aspect(aspect_char, aspect_name):\n    print(f\"\\n>>> {aspect_name}...\")\n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    ds = SimpleData(train_emb, label_matrix)\n    loader = DataLoader(ds, batch_size=256, shuffle=True)\n    \n    model = SimpleModel(1280, len(top_terms)).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(10):\n        model.train()\n        total = 0\n        for x, y in tqdm(loader, leave=False):\n            x, y = x.to(device), y.to(device)\n            opt.zero_grad()\n            loss = loss_fn(model(x), y)\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        print(f\"Epoch {epoch+1}: {total/len(loader):.4f}\")\n    \n    torch.save(model.state_dict(), f\"model_simple_{aspect_char}.pth\")\n    return top_terms\n\nresults = {c: train_aspect(c, n) for c, n in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]}\n\n# PREDICT\nprint(\"\\n>>> Predicting...\")\nwith open(\"submission_simple.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        terms = results[char]\n        model = SimpleModel(1280, len(terms)).to(device)\n        model.load_state_dict(torch.load(f\"model_simple_{char}.pth\"))\n        model.eval()\n        \n        loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n        preds = []\n        with torch.no_grad():\n            for x in tqdm(loader):\n                preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n        \n        all_preds = np.vstack(preds)\n        for i, pid in enumerate(test_ids):\n            top_idx = np.argpartition(all_preds[i], -70)[-70:]\n            for idx in top_idx:\n                if all_preds[i, idx] > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{all_preds[i, idx]:.3f}\\n\")\n        \n        del model, preds\n        gc.collect()\n\nprint(\"Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T01:16:46.234744Z","iopub.execute_input":"2025-12-05T01:16:46.235065Z","iopub.status.idle":"2025-12-05T01:19:02.789695Z","shell.execute_reply.started":"2025-12-05T01:16:46.235043Z","shell.execute_reply":"2025-12-05T01:19:02.788701Z"}},"outputs":[{"name":"stdout","text":"Loading...\n\n>>> Function...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 112061/112061 [00:03<00:00, 30319.87it/s]\n                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: 0.0174\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: 0.0037\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: 0.0035\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: 0.0029\n\n>>> Process...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 143554/143554 [00:05<00:00, 28520.60it/s]\n                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: 0.0211\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: 0.0077\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: 0.0064\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: 0.0063\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: 0.0061\n\n>>> Component...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 154977/154977 [00:05<00:00, 29307.59it/s]\n                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: 0.0188\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: 0.0057\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: 0.0050\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: 0.0049\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: 0.0046\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: 0.0045\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: 0.0043\n\n>>> Predicting...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:01<00:00, 218.01it/s]\n100%|██████████| 220/220 [00:01<00:00, 217.01it/s]\n100%|██████████| 220/220 [00:00<00:00, 220.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install obonet -q\nimport obonet\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\nsub = {}\nwith open(\"submission_simple.tsv\") as f:\n    for line in f:\n        p, t, s = line.strip().split(\"\\t\")\n        if p not in sub: sub[p] = {}\n        sub[p][t] = float(s)\n\nwith open(\"submission_v3_baseline.tsv\", \"w\") as f:\n    for pid, preds in tqdm(sub.items()):\n        final = preds.copy()\n        q = list(preds.keys())\n        while q:\n            term = q.pop(0)\n            for par in parent_map.get(term, []):\n                if final.get(par, 0) < final[term]:\n                    final[par] = final[term]\n                    if par not in preds: q.append(par)\n        \n        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n            if s > 0.001:\n                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T01:19:12.266920Z","iopub.execute_input":"2025-12-05T01:19:12.267163Z","iopub.status.idle":"2025-12-05T01:20:22.506104Z","shell.execute_reply.started":"2025-12-05T01:19:12.267145Z","shell.execute_reply":"2025-12-05T01:20:22.505038Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:55<00:00, 4061.82it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}