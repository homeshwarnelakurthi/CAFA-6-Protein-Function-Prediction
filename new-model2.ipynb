{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- MASTER TRAINING CELL (MFO - Molecular Function) ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\n# 1. LOAD & CLEAN INPUTS\nprint(\"1. Loading & Cleaning Inputs...\")\ntrain_emb = np.load(\"/kaggle/input/emb-models-ttt/train_embeds.npy\").astype(np.float32)\ntrain_ids = np.load(\"/kaggle/input/emb-models-ttt/train_ids.npy\")\ntest_emb = np.load(\"/kaggle/input/emb-models-ttt/test_embeds.npy\").astype(np.float32)\ntest_ids = np.load(\"/kaggle/input/emb-models-ttt/test_ids.npy\")\n\n# Standard Scale Inputs (Crucial for convergence)\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\nprint(f\"   Inputs ready. Shape: {train_emb.shape}\")\n\n# 2. LOAD & BUILD LABELS\nprint(\"2. Building Labels (Target: Function 'F')...\")\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n\n# Filter for Molecular Function (F)\n# Note: In this file, aspect is 'F', not 'MFO'\nTARGET_ASPECT = 'F' \naspect_terms = terms_df[terms_df['aspect'] == TARGET_ASPECT]\n\n# Get Top 1500 Terms\ntop_terms = aspect_terms['term'].value_counts().index[:1500].tolist()\nterm_map = {t: i for i, t in enumerate(top_terms)}\nnum_classes = len(top_terms)\n\n# Build Matrix\nlabel_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\nid_map = {pid: i for i, pid in enumerate(train_ids)}\nrelevant_rows = aspect_terms[aspect_terms['term'].isin(top_terms)]\n\nfor _, row in tqdm(relevant_rows.iterrows(), total=len(relevant_rows), desc=\"Mapping Labels\"):\n    if row['id'] in id_map:\n        label_matrix[id_map[row['id']], term_map[row['term']]] = 1.0\n\nprint(f\"   Labels ready. Shape: {label_matrix.shape}\")\n\n# 3. DEFINE MODEL (Simple & Robust)\nclass CAFA_MLP(nn.Module):\n    def __init__(self, n_features, n_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# 4. TRAIN\nprint(\"3. Starting Training...\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset\nclass ProteinData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return self.X[i], self.y[i]\n\n# Split 90/10\nperm = np.random.permutation(len(train_ids))\nsplit = int(len(train_ids) * 0.9)\ntrain_ds = ProteinData(train_emb[perm[:split]], label_matrix[perm[:split]])\nval_ds = ProteinData(train_emb[perm[split:]], label_matrix[perm[split:]])\n\ntrain_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n# Init\nmodel = CAFA_MLP(1280, num_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Loop\nfor epoch in range(15):\n    model.train()\n    total_loss = 0\n    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad()\n        pred = model(x)\n        loss = loss_fn(pred, y)\n        loss.backward()\n        opt.step()\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n\n# Save\ntorch.save(model.state_dict(), \"model_MFO.pth\")\nprint(\"Training Complete. Model Saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:22:29.636962Z","iopub.execute_input":"2025-11-30T06:22:29.637621Z","iopub.status.idle":"2025-11-30T06:22:55.491671Z","shell.execute_reply.started":"2025-11-30T06:22:29.637601Z","shell.execute_reply":"2025-11-30T06:22:55.490985Z"}},"outputs":[{"name":"stdout","text":"1. Loading & Cleaning Inputs...\n   Inputs ready. Shape: (82404, 1280)\n2. Building Labels (Target: Function 'F')...\n","output_type":"stream"},{"name":"stderr","text":"Mapping Labels: 100%|██████████| 112061/112061 [00:04<00:00, 23774.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"   Labels ready. Shape: (82404, 1500)\n3. Starting Training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 290/290 [00:01<00:00, 239.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0186\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 290/290 [00:01<00:00, 256.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0045\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 290/290 [00:01<00:00, 249.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0041\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 290/290 [00:01<00:00, 226.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0038\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 290/290 [00:01<00:00, 253.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0035\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 290/290 [00:01<00:00, 257.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 290/290 [00:01<00:00, 232.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 290/290 [00:01<00:00, 260.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|██████████| 290/290 [00:01<00:00, 255.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|██████████| 290/290 [00:01<00:00, 249.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0029\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|██████████| 290/290 [00:01<00:00, 217.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 Loss: 0.0028\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|██████████| 290/290 [00:01<00:00, 248.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 Loss: 0.0028\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|██████████| 290/290 [00:01<00:00, 248.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 Loss: 0.0027\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|██████████| 290/290 [00:01<00:00, 250.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 Loss: 0.0027\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|██████████| 290/290 [00:01<00:00, 218.40it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 15 Loss: 0.0026\nTraining Complete. Model Saved!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# --- GENERATE SUBMISSION ---\nprint(\"Generating Submission...\")\nmodel.eval()\n\npreds = []\nbatch_size = 1024 # Fast inference\ntest_loader = DataLoader(torch.from_numpy(test_emb), batch_size=batch_size)\n\nwith torch.no_grad():\n    for x in tqdm(test_loader, desc=\"Predicting\"):\n        x = x.to(device)\n        # Forward pass\n        logits = model(x)\n        # Sigmoid to get probabilities (0 to 1)\n        probs = torch.sigmoid(logits).cpu().numpy()\n        preds.append(probs)\n\n# Combine all batches\nall_probs = np.vstack(preds)\nprint(f\"Predictions Shape: {all_probs.shape}\") # (Test_Size, 1500)\n\n# Write to TSV file\nprint(\"Writing submission.tsv...\")\nwith open(\"submission.tsv\", \"w\") as f:\n    # Header is not strictly needed for CAFA but good practice\n    # Format: ProteinID <tab> GO_Term <tab> Score\n    \n    for i, pid in enumerate(tqdm(test_ids)):\n        # Get top 50 predictions per protein to save space\n        # (Most proteins only have a few functions)\n        row_probs = all_probs[i]\n        # Get indices of top 50 scores\n        top_indices = np.argpartition(row_probs, -50)[-50:]\n        \n        for idx in top_indices:\n            score = row_probs[idx]\n            # Only keep scores > 0.01 (filtering low confidence)\n            if score > 0.01:\n                term = top_terms[idx]\n                # Format: <ProteinID> <GO_Term> <Score>\n                f.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n\nprint(\"✅ submission.tsv created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:24:01.851179Z","iopub.execute_input":"2025-11-30T06:24:01.851701Z","iopub.status.idle":"2025-11-30T06:24:34.108130Z","shell.execute_reply.started":"2025-11-30T06:24:01.851668Z","shell.execute_reply":"2025-11-30T06:24:34.107374Z"}},"outputs":[{"name":"stdout","text":"Generating Submission...\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 220/220 [00:02<00:00, 102.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predictions Shape: (224309, 1500)\nWriting submission.tsv...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:29<00:00, 7553.02it/s]","output_type":"stream"},{"name":"stdout","text":"✅ submission.tsv created!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- MASTER SCRIPT: TRAIN ALL 3 MODELS & SUBMIT ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\nimport os\n\n# CONFIGURATION\nCONFIG = {\n    \"batch_size\": 256,\n    \"lr\": 1e-3,\n    \"epochs\": 10,  # 10 Epochs per model is enough\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/emb-models-ttt/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/emb-models-ttt/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/emb-models-ttt/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/emb-models-ttt/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\n    }\n}\n\n# --- 1. LOAD & CLEAN INPUTS (Do this ONCE) ---\nprint(\"=== 1. Loading & Cleaning Data ===\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\n# Standard Scale\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nprint(f\"Data Ready: {train_emb.shape}\")\n\n# Load Terms File\nprint(\"Loading Terms File...\")\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n\n# --- MODEL DEFINITION ---\nclass CAFA_MLP(nn.Module):\n    def __init__(self, n_features, n_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_classes)\n        )\n    def forward(self, x): return self.net(x)\n\nclass ProteinData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return self.X[i], self.y[i]\n\n# --- TRAINING FUNCTION ---\ndef train_aspect(aspect_char, aspect_name):\n    print(f\"\\n=== TRAINING MODEL FOR: {aspect_name} ({aspect_char}) ===\")\n    \n    # 1. Filter Terms\n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    # Take Top 1500 terms\n    top_terms = aspect_terms['term'].value_counts().index[:1500].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    num_classes = len(top_terms)\n    print(f\"Selected {num_classes} most common terms for {aspect_name}\")\n    \n    # 2. Build Labels\n    print(\"Building Label Matrix...\")\n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    id_map = {pid: i for i, pid in enumerate(train_ids)}\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Mapping\"):\n        if row['id'] in id_map:\n            label_matrix[id_map[row['id']], term_map[row['term']]] = 1.0\n            \n    # 3. Dataloaders\n    perm = np.random.permutation(len(train_ids))\n    split = int(len(train_ids) * 0.9)\n    train_ds = ProteinData(train_emb[perm[:split]], label_matrix[perm[:split]])\n    train_loader = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n    \n    # 4. Train\n    model = CAFA_MLP(1280, num_classes).to(CONFIG[\"device\"])\n    opt = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(CONFIG[\"epochs\"]):\n        model.train()\n        total_loss = 0\n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n            opt.zero_grad()\n            loss = loss_fn(model(x), y)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n        \n    # Save Model\n    save_name = f\"model_{aspect_char}.pth\"\n    torch.save(model.state_dict(), save_name)\n    print(f\"Saved {save_name}\")\n    \n    return top_terms, save_name\n\n# --- RUN TRAINING FOR ALL 3 ASPECTS ---\n# Train MFO (Function)\ntop_terms_F, model_path_F = train_aspect('F', 'Molecular Function')\n# Train BPO (Process)\ntop_terms_P, model_path_P = train_aspect('P', 'Biological Process')\n# Train CCO (Component)\ntop_terms_C, model_path_C = train_aspect('C', 'Cellular Component')\n\n# --- GENERATE SUBMISSION ---\nprint(\"\\n=== GENERATING FINAL SUBMISSION ===\")\n\nmodels_info = [\n    ('F', model_path_F, top_terms_F),\n    ('P', model_path_P, top_terms_P),\n    ('C', model_path_C, top_terms_C)\n]\n\nwith open(\"submission_combined.tsv\", \"w\") as f:\n    # Loop through each trained model\n    for aspect, path, terms in models_info:\n        print(f\"Predicting {aspect}...\")\n        \n        # Load Model\n        num_classes = len(terms)\n        model = CAFA_MLP(1280, num_classes).to(CONFIG[\"device\"])\n        model.load_state_dict(torch.load(path))\n        model.eval()\n        \n        # Predict\n        test_loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n        preds = []\n        with torch.no_grad():\n            for x in tqdm(test_loader):\n                x = x.to(CONFIG[\"device\"])\n                probs = torch.sigmoid(model(x)).cpu().numpy()\n                preds.append(probs)\n        \n        all_probs = np.vstack(preds)\n        \n        # Write High Confidence Predictions\n        print(f\"Writing predictions for {aspect}...\")\n        for i, pid in enumerate(tqdm(test_ids)):\n            row_probs = all_probs[i]\n            top_indices = np.argpartition(row_probs, -50)[-50:]\n            \n            for idx in top_indices:\n                score = row_probs[idx]\n                if score > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{score:.3f}\\n\")\n        \n        # Clean RAM\n        del model, all_probs, preds\n        torch.cuda.empty_cache()\n        gc.collect()\n\nprint(\"\\n✅ DONE! Download 'submission_combined.tsv' and submit!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:32:54.275258Z","iopub.execute_input":"2025-11-30T06:32:54.275852Z","iopub.status.idle":"2025-11-30T06:35:25.916555Z","shell.execute_reply.started":"2025-11-30T06:32:54.275822Z","shell.execute_reply":"2025-11-30T06:35:25.915978Z"}},"outputs":[{"name":"stdout","text":"=== 1. Loading & Cleaning Data ===\nData Ready: (82404, 1280)\nLoading Terms File...\n\n=== TRAINING MODEL FOR: Molecular Function (F) ===\nSelected 1500 most common terms for Molecular Function\nBuilding Label Matrix...\n","output_type":"stream"},{"name":"stderr","text":"Mapping: 100%|██████████| 112061/112061 [00:04<00:00, 23801.83it/s]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0186\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0045\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0041\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0038\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0034\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0029\nSaved model_F.pth\n\n=== TRAINING MODEL FOR: Biological Process (P) ===\nSelected 1500 most common terms for Biological Process\nBuilding Label Matrix...\n","output_type":"stream"},{"name":"stderr","text":"Mapping: 100%|██████████| 143554/143554 [00:05<00:00, 24191.62it/s]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0226\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0078\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0064\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0063\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0060\nSaved model_P.pth\n\n=== TRAINING MODEL FOR: Cellular Component (C) ===\nSelected 1500 most common terms for Cellular Component\nBuilding Label Matrix...\n","output_type":"stream"},{"name":"stderr","text":"Mapping: 100%|██████████| 154977/154977 [00:06<00:00, 24530.34it/s]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0201\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0057\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0051\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0049\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0046\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0045\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0043\nSaved model_C.pth\n\n=== GENERATING FINAL SUBMISSION ===\nPredicting F...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:01<00:00, 115.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing predictions for F...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:27<00:00, 8235.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predicting P...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:01<00:00, 157.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing predictions for P...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:30<00:00, 7254.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predicting C...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:01<00:00, 152.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing predictions for C...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:30<00:00, 7344.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ DONE! Download 'submission_combined.tsv' and submit!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install obonet networkx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:54:09.884322Z","iopub.execute_input":"2025-11-30T06:54:09.885109Z","iopub.status.idle":"2025-11-30T06:54:14.355507Z","shell.execute_reply.started":"2025-11-30T06:54:09.885081Z","shell.execute_reply":"2025-11-30T06:54:14.354333Z"}},"outputs":[{"name":"stdout","text":"Collecting obonet\n  Downloading obonet-1.1.1-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\nDownloading obonet-1.1.1-py3-none-any.whl (9.2 kB)\nInstalling collected packages: obonet\nSuccessfully installed obonet-1.1.1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# --- ADVANCED POST-PROCESSING: GRAPH PROPAGATION ---\nimport networkx\nimport obonet\nimport pandas as pd\nfrom tqdm import tqdm\n\nprint(\"Loading Gene Ontology Graph (go-basic.obo)...\")\n# Load the graph\ngraph = obonet.read_obo(\"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\")\n\n# Create a mapping of Child -> Parents\n# We only care about 'is_a' relationships for propagation\nprint(\"Building Parent Map...\")\nparent_map = {}\nfor node in graph.nodes():\n    parents = [p for p in graph.successors(node) if p in graph.nodes()]\n    if parents:\n        parent_map[node] = parents\n\nprint(f\"Graph loaded. {len(parent_map)} terms have parents.\")\n\n# Load your current submission\nprint(\"Loading your submission...\")\n# We read it into a Dictionary for fast lookup: {ProteinID: {Term: Score}}\nsubmission_dict = {}\n\n# Read line by line to save RAM\nwith open(\"submission_combined.tsv\", \"r\") as f:\n    for line in tqdm(f):\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 3:\n            pid, term, score = parts\n            score = float(score)\n            \n            if pid not in submission_dict:\n                submission_dict[pid] = {}\n            submission_dict[pid][term] = score\n\nprint(f\"Loaded predictions for {len(submission_dict)} proteins.\")\n\n# PROPAGATION LOGIC\nprint(\"Propagating Scores (This is the Magic Step)...\")\n# Open output file\nwith open(\"submission_propagated.tsv\", \"w\") as f_out:\n    \n    for pid, preds in tqdm(submission_dict.items()):\n        # preds is {Term: Score}\n        # We need to propagate scores UP the tree\n        # If Child has score 0.9, Parent gets max(Parent_Score, 0.9)\n        \n        # We iterate multiple times to propagate up multiple levels\n        # (A -> B -> C). A naive single pass might miss C if processed in wrong order.\n        # A \"Set\" based approach is faster:\n        \n        # 1. Get all terms currently predicted\n        active_terms = set(preds.keys())\n        \n        # 2. Add all ancestors of these terms\n        final_scores = preds.copy() # Start with current scores\n        \n        queue = list(active_terms)\n        visited = set(queue)\n        \n        while queue:\n            term = queue.pop(0)\n            current_score = final_scores.get(term, 0.0)\n            \n            # Get parents\n            if term in parent_map:\n                for parent in parent_map[term]:\n                    # Parent score is at least Child score\n                    old_parent_score = final_scores.get(parent, 0.0)\n                    new_parent_score = max(old_parent_score, current_score)\n                    \n                    if new_parent_score > old_parent_score:\n                        final_scores[parent] = new_parent_score\n                        if parent not in visited:\n                            queue.append(parent)\n                            visited.add(parent)\n        \n        # 3. Write to file (Top 70 to include propagated parents)\n        # Sort by score\n        sorted_terms = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:70]\n        \n        for term, score in sorted_terms:\n            # Only confident ones\n            if score > 0.001: \n                f_out.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n\nprint(\"✅ submission_propagated.tsv created! This is your 'Senior Scientist' submission.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:54:59.143073Z","iopub.execute_input":"2025-11-30T06:54:59.143383Z","iopub.status.idle":"2025-11-30T06:56:54.823450Z","shell.execute_reply.started":"2025-11-30T06:54:59.143357Z","shell.execute_reply":"2025-11-30T06:56:54.822733Z"}},"outputs":[{"name":"stdout","text":"Loading Gene Ontology Graph (go-basic.obo)...\nBuilding Parent Map...\nGraph loaded. 40119 terms have parents.\nLoading your submission...\n","output_type":"stream"},{"name":"stderr","text":"14941910it [00:16, 904825.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded predictions for 224309 proteins.\nPropagating Scores (This is the Magic Step)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [01:33<00:00, 2395.81it/s]","output_type":"stream"},{"name":"stdout","text":"✅ submission_propagated.tsv created! This is your 'Senior Scientist' submission.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}