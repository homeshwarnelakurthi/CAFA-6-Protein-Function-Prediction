{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":13927528,"sourceType":"datasetVersion","datasetId":8875178}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- MASTER TRAINING CELL (MFO - Molecular Function) ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\n\n# 1. LOAD & CLEAN INPUTS\nprint(\"1. Loading & Cleaning Inputs...\")\ntrain_emb = np.load(\"/kaggle/input/emb-models-ttt/train_embeds.npy\").astype(np.float32)\ntrain_ids = np.load(\"/kaggle/input/emb-models-ttt/train_ids.npy\")\ntest_emb = np.load(\"/kaggle/input/emb-models-ttt/test_embeds.npy\").astype(np.float32)\ntest_ids = np.load(\"/kaggle/input/emb-models-ttt/test_ids.npy\")\n\n# Standard Scale Inputs (Crucial for convergence)\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\nprint(f\"   Inputs ready. Shape: {train_emb.shape}\")\n\n# 2. LOAD & BUILD LABELS\nprint(\"2. Building Labels (Target: Function 'F')...\")\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n\n# Filter for Molecular Function (F)\n# Note: In this file, aspect is 'F', not 'MFO'\nTARGET_ASPECT = 'F' \naspect_terms = terms_df[terms_df['aspect'] == TARGET_ASPECT]\n\n# Get Top 1500 Terms\ntop_terms = aspect_terms['term'].value_counts().index[:1500].tolist()\nterm_map = {t: i for i, t in enumerate(top_terms)}\nnum_classes = len(top_terms)\n\n# Build Matrix\nlabel_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\nid_map = {pid: i for i, pid in enumerate(train_ids)}\nrelevant_rows = aspect_terms[aspect_terms['term'].isin(top_terms)]\n\nfor _, row in tqdm(relevant_rows.iterrows(), total=len(relevant_rows), desc=\"Mapping Labels\"):\n    if row['id'] in id_map:\n        label_matrix[id_map[row['id']], term_map[row['term']]] = 1.0\n\nprint(f\"   Labels ready. Shape: {label_matrix.shape}\")\n\n# 3. DEFINE MODEL (Simple & Robust)\nclass CAFA_MLP(nn.Module):\n    def __init__(self, n_features, n_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_classes)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# 4. TRAIN\nprint(\"3. Starting Training...\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset\nclass ProteinData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return self.X[i], self.y[i]\n\n# Split 90/10\nperm = np.random.permutation(len(train_ids))\nsplit = int(len(train_ids) * 0.9)\ntrain_ds = ProteinData(train_emb[perm[:split]], label_matrix[perm[:split]])\nval_ds = ProteinData(train_emb[perm[split:]], label_matrix[perm[split:]])\n\ntrain_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=256)\n\n# Init\nmodel = CAFA_MLP(1280, num_classes).to(device)\nopt = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Loop\nfor epoch in range(15):\n    model.train()\n    total_loss = 0\n    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad()\n        pred = model(x)\n        loss = loss_fn(pred, y)\n        loss.backward()\n        opt.step()\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n\n# Save\ntorch.save(model.state_dict(), \"model_MFO.pth\")\nprint(\"Training Complete. Model Saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:22:29.636962Z","iopub.execute_input":"2025-11-30T06:22:29.637621Z","iopub.status.idle":"2025-11-30T06:22:55.491671Z","shell.execute_reply.started":"2025-11-30T06:22:29.637601Z","shell.execute_reply":"2025-11-30T06:22:55.490985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- GENERATE SUBMISSION ---\nprint(\"Generating Submission...\")\nmodel.eval()\n\npreds = []\nbatch_size = 1024 # Fast inference\ntest_loader = DataLoader(torch.from_numpy(test_emb), batch_size=batch_size)\n\nwith torch.no_grad():\n    for x in tqdm(test_loader, desc=\"Predicting\"):\n        x = x.to(device)\n        # Forward pass\n        logits = model(x)\n        # Sigmoid to get probabilities (0 to 1)\n        probs = torch.sigmoid(logits).cpu().numpy()\n        preds.append(probs)\n\n# Combine all batches\nall_probs = np.vstack(preds)\nprint(f\"Predictions Shape: {all_probs.shape}\") # (Test_Size, 1500)\n\n# Write to TSV file\nprint(\"Writing submission.tsv...\")\nwith open(\"submission.tsv\", \"w\") as f:\n    # Header is not strictly needed for CAFA but good practice\n    # Format: ProteinID <tab> GO_Term <tab> Score\n    \n    for i, pid in enumerate(tqdm(test_ids)):\n        # Get top 50 predictions per protein to save space\n        # (Most proteins only have a few functions)\n        row_probs = all_probs[i]\n        # Get indices of top 50 scores\n        top_indices = np.argpartition(row_probs, -50)[-50:]\n        \n        for idx in top_indices:\n            score = row_probs[idx]\n            # Only keep scores > 0.01 (filtering low confidence)\n            if score > 0.01:\n                term = top_terms[idx]\n                # Format: <ProteinID> <GO_Term> <Score>\n                f.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n\nprint(\"✅ submission.tsv created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:24:01.851179Z","iopub.execute_input":"2025-11-30T06:24:01.851701Z","iopub.status.idle":"2025-11-30T06:24:34.108130Z","shell.execute_reply.started":"2025-11-30T06:24:01.851668Z","shell.execute_reply":"2025-11-30T06:24:34.107374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- MASTER SCRIPT: TRAIN ALL 3 MODELS & SUBMIT ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\nimport os\n\n# CONFIGURATION\nCONFIG = {\n    \"batch_size\": 256,\n    \"lr\": 1e-3,\n    \"epochs\": 10,  # 10 Epochs per model is enough\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/emb-models-ttt/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/emb-models-ttt/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/emb-models-ttt/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/emb-models-ttt/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\n    }\n}\n\n# --- 1. LOAD & CLEAN INPUTS (Do this ONCE) ---\nprint(\"=== 1. Loading & Cleaning Data ===\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\n# Standard Scale\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nprint(f\"Data Ready: {train_emb.shape}\")\n\n# Load Terms File\nprint(\"Loading Terms File...\")\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n\n# --- MODEL DEFINITION ---\nclass CAFA_MLP(nn.Module):\n    def __init__(self, n_features, n_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_classes)\n        )\n    def forward(self, x): return self.net(x)\n\nclass ProteinData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return self.X[i], self.y[i]\n\n# --- TRAINING FUNCTION ---\ndef train_aspect(aspect_char, aspect_name):\n    print(f\"\\n=== TRAINING MODEL FOR: {aspect_name} ({aspect_char}) ===\")\n    \n    # 1. Filter Terms\n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    # Take Top 1500 terms\n    top_terms = aspect_terms['term'].value_counts().index[:1500].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    num_classes = len(top_terms)\n    print(f\"Selected {num_classes} most common terms for {aspect_name}\")\n    \n    # 2. Build Labels\n    print(\"Building Label Matrix...\")\n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    id_map = {pid: i for i, pid in enumerate(train_ids)}\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Mapping\"):\n        if row['id'] in id_map:\n            label_matrix[id_map[row['id']], term_map[row['term']]] = 1.0\n            \n    # 3. Dataloaders\n    perm = np.random.permutation(len(train_ids))\n    split = int(len(train_ids) * 0.9)\n    train_ds = ProteinData(train_emb[perm[:split]], label_matrix[perm[:split]])\n    train_loader = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n    \n    # 4. Train\n    model = CAFA_MLP(1280, num_classes).to(CONFIG[\"device\"])\n    opt = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(CONFIG[\"epochs\"]):\n        model.train()\n        total_loss = 0\n        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n            opt.zero_grad()\n            loss = loss_fn(model(x), y)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n        \n    # Save Model\n    save_name = f\"model_{aspect_char}.pth\"\n    torch.save(model.state_dict(), save_name)\n    print(f\"Saved {save_name}\")\n    \n    return top_terms, save_name\n\n# --- RUN TRAINING FOR ALL 3 ASPECTS ---\n# Train MFO (Function)\ntop_terms_F, model_path_F = train_aspect('F', 'Molecular Function')\n# Train BPO (Process)\ntop_terms_P, model_path_P = train_aspect('P', 'Biological Process')\n# Train CCO (Component)\ntop_terms_C, model_path_C = train_aspect('C', 'Cellular Component')\n\n# --- GENERATE SUBMISSION ---\nprint(\"\\n=== GENERATING FINAL SUBMISSION ===\")\n\nmodels_info = [\n    ('F', model_path_F, top_terms_F),\n    ('P', model_path_P, top_terms_P),\n    ('C', model_path_C, top_terms_C)\n]\n\nwith open(\"submission_combined.tsv\", \"w\") as f:\n    # Loop through each trained model\n    for aspect, path, terms in models_info:\n        print(f\"Predicting {aspect}...\")\n        \n        # Load Model\n        num_classes = len(terms)\n        model = CAFA_MLP(1280, num_classes).to(CONFIG[\"device\"])\n        model.load_state_dict(torch.load(path))\n        model.eval()\n        \n        # Predict\n        test_loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n        preds = []\n        with torch.no_grad():\n            for x in tqdm(test_loader):\n                x = x.to(CONFIG[\"device\"])\n                probs = torch.sigmoid(model(x)).cpu().numpy()\n                preds.append(probs)\n        \n        all_probs = np.vstack(preds)\n        \n        # Write High Confidence Predictions\n        print(f\"Writing predictions for {aspect}...\")\n        for i, pid in enumerate(tqdm(test_ids)):\n            row_probs = all_probs[i]\n            top_indices = np.argpartition(row_probs, -50)[-50:]\n            \n            for idx in top_indices:\n                score = row_probs[idx]\n                if score > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{score:.3f}\\n\")\n        \n        # Clean RAM\n        del model, all_probs, preds\n        torch.cuda.empty_cache()\n        gc.collect()\n\nprint(\"\\n✅ DONE! Download 'submission_combined.tsv' and submit!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:32:54.275258Z","iopub.execute_input":"2025-11-30T06:32:54.275852Z","iopub.status.idle":"2025-11-30T06:35:25.916555Z","shell.execute_reply.started":"2025-11-30T06:32:54.275822Z","shell.execute_reply":"2025-11-30T06:35:25.915978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install obonet networkx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:54:09.884322Z","iopub.execute_input":"2025-11-30T06:54:09.885109Z","iopub.status.idle":"2025-11-30T06:54:14.355507Z","shell.execute_reply.started":"2025-11-30T06:54:09.885081Z","shell.execute_reply":"2025-11-30T06:54:14.354333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- ADVANCED POST-PROCESSING: GRAPH PROPAGATION ---\nimport networkx\nimport obonet\nimport pandas as pd\nfrom tqdm import tqdm\n\nprint(\"Loading Gene Ontology Graph (go-basic.obo)...\")\n# Load the graph\ngraph = obonet.read_obo(\"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\")\n\n# Create a mapping of Child -> Parents\n# We only care about 'is_a' relationships for propagation\nprint(\"Building Parent Map...\")\nparent_map = {}\nfor node in graph.nodes():\n    parents = [p for p in graph.successors(node) if p in graph.nodes()]\n    if parents:\n        parent_map[node] = parents\n\nprint(f\"Graph loaded. {len(parent_map)} terms have parents.\")\n\n# Load your current submission\nprint(\"Loading your submission...\")\n# We read it into a Dictionary for fast lookup: {ProteinID: {Term: Score}}\nsubmission_dict = {}\n\n# Read line by line to save RAM\nwith open(\"submission_combined.tsv\", \"r\") as f:\n    for line in tqdm(f):\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 3:\n            pid, term, score = parts\n            score = float(score)\n            \n            if pid not in submission_dict:\n                submission_dict[pid] = {}\n            submission_dict[pid][term] = score\n\nprint(f\"Loaded predictions for {len(submission_dict)} proteins.\")\n\n# PROPAGATION LOGIC\nprint(\"Propagating Scores (This is the Magic Step)...\")\n# Open output file\nwith open(\"submission_propagated.tsv\", \"w\") as f_out:\n    \n    for pid, preds in tqdm(submission_dict.items()):\n        # preds is {Term: Score}\n        # We need to propagate scores UP the tree\n        # If Child has score 0.9, Parent gets max(Parent_Score, 0.9)\n        \n        # We iterate multiple times to propagate up multiple levels\n        # (A -> B -> C). A naive single pass might miss C if processed in wrong order.\n        # A \"Set\" based approach is faster:\n        \n        # 1. Get all terms currently predicted\n        active_terms = set(preds.keys())\n        \n        # 2. Add all ancestors of these terms\n        final_scores = preds.copy() # Start with current scores\n        \n        queue = list(active_terms)\n        visited = set(queue)\n        \n        while queue:\n            term = queue.pop(0)\n            current_score = final_scores.get(term, 0.0)\n            \n            # Get parents\n            if term in parent_map:\n                for parent in parent_map[term]:\n                    # Parent score is at least Child score\n                    old_parent_score = final_scores.get(parent, 0.0)\n                    new_parent_score = max(old_parent_score, current_score)\n                    \n                    if new_parent_score > old_parent_score:\n                        final_scores[parent] = new_parent_score\n                        if parent not in visited:\n                            queue.append(parent)\n                            visited.add(parent)\n        \n        # 3. Write to file (Top 70 to include propagated parents)\n        # Sort by score\n        sorted_terms = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:70]\n        \n        for term, score in sorted_terms:\n            # Only confident ones\n            if score > 0.001: \n                f_out.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n\nprint(\"✅ submission_propagated.tsv created! This is your 'Senior Scientist' submission.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:54:59.143073Z","iopub.execute_input":"2025-11-30T06:54:59.143383Z","iopub.status.idle":"2025-11-30T06:56:54.823450Z","shell.execute_reply.started":"2025-11-30T06:54:59.143357Z","shell.execute_reply":"2025-11-30T06:56:54.822733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- DIAMOND ENSEMBLE TRAINING ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport gc\n\n# Define 3 Different Architectures\nclass Model_Standard(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass Model_Deep(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 1024), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass Model_HighDropout(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512), nn.ReLU(), nn.Dropout(0.6), # High Dropout\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\n# Helper to train one specific model variant\ndef train_variant(model_class, aspect_char, aspect_name, variant_name):\n    print(f\"\\n>>> Training {variant_name} for {aspect_name}...\")\n    \n    # 1. Setup Data (Reuse existing embeddings/labels code)\n    # (Assuming you ran the Master Script before, so 'train_emb' exists)\n    \n    # Get Terms/Labels\n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:1500].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    num_classes = len(top_terms)\n    \n    # Build Labels\n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    id_map = {pid: i for i, pid in enumerate(train_ids)}\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    for _, row in relevant.iterrows():\n        if row['id'] in id_map:\n            label_matrix[id_map[row['id']], term_map[row['term']]] = 1.0\n            \n    # Dataloader\n    perm = np.random.permutation(len(train_ids))\n    ds = ProteinData(train_emb[perm], label_matrix[perm])\n    loader = DataLoader(ds, batch_size=512, shuffle=True)\n    \n    # Train\n    model = model_class(1280, num_classes).to(CONFIG[\"device\"])\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(8): # 8 Epochs is enough for ensemble\n        model.train()\n        for x, y in loader:\n            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n            opt.zero_grad()\n            loss = loss_fn(model(x), y)\n            loss.backward()\n            opt.step()\n            \n    # Save\n    torch.save(model.state_dict(), f\"model_{aspect_char}_{variant_name}.pth\")\n    return top_terms\n\n# --- EXECUTE ENSEMBLE ---\nvariants = [\n    (\"Standard\", Model_Standard),\n    (\"Deep\", Model_Deep),\n    (\"Dropout\", Model_HighDropout)\n]\n\naspects = [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]\n\n# Train everything (Total 9 models)\nall_terms = {}\nfor aspect_char, aspect_name in aspects:\n    for var_name, var_class in variants:\n        terms = train_variant(var_class, aspect_char, aspect_name, var_name)\n        all_terms[aspect_char] = terms # Save term list\n\n# --- PREDICT & AVERAGE ---\nprint(\"\\n>>> Generating Ensemble Predictions...\")\n\nwith open(\"submission_ensemble.tsv\", \"w\") as f:\n    test_loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n    \n    for aspect_char, aspect_name in aspects:\n        print(f\"Ensembling {aspect_name}...\")\n        terms = all_terms[aspect_char]\n        num_classes = len(terms)\n        \n        # Load all 3 models for this aspect\n        models = []\n        for var_name, var_class in variants:\n            m = var_class(1280, num_classes).to(CONFIG[\"device\"])\n            m.load_state_dict(torch.load(f\"model_{aspect_char}_{var_name}.pth\"))\n            m.eval()\n            models.append(m)\n            \n        # Predict and Average\n        all_probs = []\n        with torch.no_grad():\n            for x in tqdm(test_loader):\n                x = x.to(CONFIG[\"device\"])\n                # Get predictions from all 3\n                p1 = torch.sigmoid(models[0](x))\n                p2 = torch.sigmoid(models[1](x))\n                p3 = torch.sigmoid(models[2](x))\n                \n                # AVERAGE THEM\n                avg_p = (p1 + p2 + p3) / 3.0\n                all_probs.append(avg_p.cpu().numpy())\n                \n        final_probs = np.vstack(all_probs)\n        \n        # Write to file\n        for i, pid in enumerate(test_ids):\n            row = final_probs[i]\n            top_idx = np.argpartition(row, -50)[-50:]\n            for idx in top_idx:\n                score = row[idx]\n                if score > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{score:.3f}\\n\")\n\nprint(\"✅ submission_ensemble.tsv created! Submit this to crush 0.20!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:08:22.776037Z","iopub.execute_input":"2025-11-30T07:08:22.776865Z","iopub.status.idle":"2025-11-30T07:12:09.709714Z","shell.execute_reply.started":"2025-11-30T07:08:22.776840Z","shell.execute_reply":"2025-11-30T07:12:09.709068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- COMPLETE RESTART: TAXONOMY-AWARE TRAINING (FIXED) ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport os\n\n# 1. CONFIG & DEVICE\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Running on: {device}\")\n\nCONFIG = {\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/emb-models-ttt/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/emb-models-ttt/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/emb-models-ttt/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/emb-models-ttt/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"train_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\",\n        \"test_tax\": \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\"\n    }\n}\n\n# 2. LOAD & CLEAN EMBEDDINGS\nprint(\"\\n=== 1. Loading Embeddings ===\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\n# Standard Scale (Critical for fast convergence)\nprint(\"Normalizing data...\")\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\nprint(\"Embeddings Ready.\")\n\n# 3. PREPARE TAXONOMY (FIXED)\nprint(\"\\n=== 2. Preparing Taxonomy Indices ===\")\ntrain_tax_df = pd.read_csv(CONFIG[\"paths\"][\"train_tax\"], sep=\"\\t\", header=None, names=[\"id\", \"tax_id\"])\ntest_tax_df = pd.read_csv(CONFIG[\"paths\"][\"test_tax\"], sep=\"\\t\", header=None, names=[\"id\", \"tax_id\"])\n\n# Force to String to avoid sorting errors\ntrain_tax_df['tax_id'] = train_tax_df['tax_id'].astype(str)\ntest_tax_df['tax_id'] = test_tax_df['tax_id'].astype(str)\n\n# Map unique taxons to integer indices [0, N]\nall_taxons = set(train_tax_df['tax_id'].unique()) | set(test_tax_df['tax_id'].unique())\ntax_list = sorted(list(all_taxons))\ntax_map = {t: i for i, t in enumerate(tax_list)}\nnum_taxons = len(tax_list)\nprint(f\"Total Unique Species: {num_taxons}\")\n\ndef get_tax_indices(id_list, df):\n    mapping = dict(zip(df[\"id\"], df[\"tax_id\"]))\n    # Default to index 0 (unknown) if missing\n    return np.array([tax_map.get(mapping.get(pid, \"0\"), 0) for pid in id_list], dtype=np.int32)\n\ntrain_tax_idx = get_tax_indices(train_ids, train_tax_df)\ntest_tax_idx = get_tax_indices(test_ids, test_tax_df)\nprint(\"Taxonomy Indices Ready.\")\n\n# 4. DEFINE TAXONOMY MODEL\nclass TaxModel(nn.Module):\n    def __init__(self, n_feat, n_taxons, n_class):\n        super().__init__()\n        # Learnable embedding for each species\n        self.tax_emb = nn.Embedding(n_taxons, 64)\n        \n        # Input is Sequence(1280) + Species(64)\n        self.net = nn.Sequential(\n            nn.Linear(n_feat + 64, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n        \n    def forward(self, x, t):\n        t_vec = self.tax_emb(t)\n        combined = torch.cat([x, t_vec], dim=1)\n        return self.net(combined)\n\nclass MultiInputData(Dataset):\n    def __init__(self, X, T, y=None):\n        self.X = torch.from_numpy(X)\n        self.T = torch.from_numpy(T).long()\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i):\n        return (self.X[i], self.T[i], self.y[i]) if self.y is not None else (self.X[i], self.T[i])\n\n# 5. TRAINING LOOP\ndef train_tax_aware(aspect_char, aspect_name):\n    print(f\"\\n>>> Training Tax-Aware Model for {aspect_name} ({aspect_char})...\")\n    \n    # Load Labels\n    terms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:1500].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    num_classes = len(top_terms)\n    \n    # Build Matrix\n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    id_map = {pid: i for i, pid in enumerate(train_ids)}\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    # Fast mapping\n    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Mapping Labels\"):\n        if row['id'] in id_map:\n            label_matrix[id_map[row['id']], term_map[row['term']]] = 1.0\n            \n    # Dataloader\n    ds = MultiInputData(train_emb, train_tax_idx, label_matrix)\n    loader = DataLoader(ds, batch_size=256, shuffle=True)\n    \n    # Model Init\n    model = TaxModel(1280, num_taxons, num_classes).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    # Train\n    for epoch in range(10):\n        model.train()\n        total_loss = 0\n        for x, t, y in tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False):\n            x, t, y = x.to(device), t.to(device), y.to(device)\n            opt.zero_grad()\n            loss = loss_fn(model(x, t), y)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n            \n    torch.save(model.state_dict(), f\"model_tax_{aspect_char}.pth\")\n    return top_terms\n\n# 6. EXECUTE\ntop_terms_F = train_tax_aware('F', 'Molecular Function')\ntop_terms_P = train_tax_aware('P', 'Biological Process')\ntop_terms_C = train_tax_aware('C', 'Cellular Component')\n\nprint(\"\\n✅ All Taxonomy-Aware Models Trained!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:12:16.823203Z","iopub.execute_input":"2025-11-30T19:12:16.824062Z","iopub.status.idle":"2025-11-30T19:13:33.269882Z","shell.execute_reply.started":"2025-11-30T19:12:16.824034Z","shell.execute_reply":"2025-11-30T19:13:33.269026Z"}},"outputs":[{"name":"stdout","text":"Running on: cuda\n\n=== 1. Loading Embeddings ===\nNormalizing data...\nEmbeddings Ready.\n\n=== 2. Preparing Taxonomy Indices ===\nTotal Unique Species: 9835\nTaxonomy Indices Ready.\n\n>>> Training Tax-Aware Model for Molecular Function (F)...\n","output_type":"stream"},{"name":"stderr","text":"Mapping Labels: 100%|██████████| 112061/112061 [00:04<00:00, 24683.77it/s]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0152\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0043\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0038\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0035\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0029\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0028\n\n>>> Training Tax-Aware Model for Biological Process (P)...\n","output_type":"stream"},{"name":"stderr","text":"Mapping Labels: 100%|██████████| 143554/143554 [00:05<00:00, 24737.39it/s]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0196\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0077\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0071\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0064\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0063\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0061\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0060\n\n>>> Training Tax-Aware Model for Cellular Component (C)...\n","output_type":"stream"},{"name":"stderr","text":"Mapping Labels: 100%|██████████| 154977/154977 [00:06<00:00, 24623.12it/s]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.0169\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.0054\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.0049\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Loss: 0.0047\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Loss: 0.0045\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Loss: 0.0043\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Loss: 0.0042\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Loss: 0.0041\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Loss: 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                            ","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Loss: 0.0040\n\n✅ All Taxonomy-Aware Models Trained!\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- GENERATE TAX-AWARE SUBMISSION ---\nimport gc\n\nprint(\"\\n=== Generating Taxonomy-Aware Submission ===\")\n\nmodels_info = [\n    ('F', f\"model_tax_F.pth\", top_terms_F),\n    ('P', f\"model_tax_P.pth\", top_terms_P),\n    ('C', f\"model_tax_C.pth\", top_terms_C)\n]\n\n# Create final file\nwith open(\"submission_tax_aware.tsv\", \"w\") as f:\n    \n    for aspect, path, terms in models_info:\n        print(f\"Predicting {aspect}...\")\n        num_classes = len(terms)\n        \n        # Load Model\n        model = TaxModel(1280, num_taxons, num_classes).to(device)\n        model.load_state_dict(torch.load(path))\n        model.eval()\n        \n        # Dataset for Inference\n        # Note: We need both Embeddings AND Tax Indices\n        ds = MultiInputData(test_emb, test_tax_idx)\n        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n        \n        preds = []\n        with torch.no_grad():\n            for x, t in tqdm(loader):\n                x, t = x.to(device), t.to(device)\n                logits = model(x, t)\n                probs = torch.sigmoid(logits).cpu().numpy()\n                preds.append(probs)\n        \n        all_probs = np.vstack(preds)\n        \n        print(f\"Writing predictions for {aspect}...\")\n        for i, pid in enumerate(tqdm(test_ids)):\n            row_probs = all_probs[i]\n            # Get Top 50\n            top_indices = np.argpartition(row_probs, -50)[-50:]\n            \n            for idx in top_indices:\n                score = row_probs[idx]\n                if score > 0.01: # Only write confident predictions\n                    term = terms[idx]\n                    f.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n                    \n        # Clean RAM\n        del model, all_probs, preds\n        torch.cuda.empty_cache()\n        gc.collect()\n\nprint(\"\\n✅ submission_tax_aware.tsv created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:14:48.311134Z","iopub.execute_input":"2025-11-30T19:14:48.311690Z","iopub.status.idle":"2025-11-30T19:16:18.892360Z","shell.execute_reply.started":"2025-11-30T19:14:48.311665Z","shell.execute_reply":"2025-11-30T19:16:18.891664Z"}},"outputs":[{"name":"stdout","text":"\n=== Generating Taxonomy-Aware Submission ===\nPredicting F...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:03<00:00, 61.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Writing predictions for F...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:25<00:00, 8710.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predicting P...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:02<00:00, 92.29it/s] \n","output_type":"stream"},{"name":"stdout","text":"Writing predictions for P...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:25<00:00, 8744.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Predicting C...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 220/220 [00:02<00:00, 92.81it/s] \n","output_type":"stream"},{"name":"stdout","text":"Writing predictions for C...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224309/224309 [00:29<00:00, 7734.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ submission_tax_aware.tsv created!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install obonet networkx\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:24:56.629382Z","iopub.execute_input":"2025-11-30T19:24:56.630138Z","iopub.status.idle":"2025-11-30T19:25:00.795407Z","shell.execute_reply.started":"2025-11-30T19:24:56.630112Z","shell.execute_reply":"2025-11-30T19:25:00.794390Z"}},"outputs":[{"name":"stdout","text":"Collecting obonet\n  Downloading obonet-1.1.1-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\nDownloading obonet-1.1.1-py3-none-any.whl (9.2 kB)\nInstalling collected packages: obonet\nSuccessfully installed obonet-1.1.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- PROPAGATION FOR TAXONOMY MODEL ---\nimport networkx\nimport obonet\nimport pandas as pd\nfrom tqdm import tqdm\n\nprint(\"Loading Graph...\")\ngraph = obonet.read_obo(\"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\")\nparent_map = {}\nfor node in graph.nodes():\n    parents = [p for p in graph.successors(node) if p in graph.nodes()]\n    if parents: parent_map[node] = parents\n\nprint(\"Loading Tax-Aware Predictions...\")\nsubmission_dict = {}\n\n# USE THE FILE YOU JUST GENERATED\ninput_filename = \"submission_tax_aware.tsv\" \n\nwith open(input_filename, \"r\") as f:\n    for line in tqdm(f):\n        parts = line.strip().split(\"\\t\")\n        if len(parts) == 3:\n            pid, term, score = parts\n            score = float(score)\n            if pid not in submission_dict: submission_dict[pid] = {}\n            submission_dict[pid][term] = score\n\nprint(\"Propagating...\")\nwith open(\"submission_tax_prop.tsv\", \"w\") as f_out:\n    for pid, preds in tqdm(submission_dict.items()):\n        active_terms = set(preds.keys())\n        final_scores = preds.copy()\n        queue = list(active_terms)\n        visited = set(queue)\n        \n        while queue:\n            term = queue.pop(0)\n            current_score = final_scores.get(term, 0.0)\n            if term in parent_map:\n                for parent in parent_map[term]:\n                    old_score = final_scores.get(parent, 0.0)\n                    new_score = max(old_score, current_score)\n                    if new_score > old_score:\n                        final_scores[parent] = new_score\n                        if parent not in visited:\n                            queue.append(parent)\n                            visited.add(parent)\n        \n        # Write Top 70\n        sorted_terms = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:70]\n        for term, score in sorted_terms:\n            if score > 0.001:\n                f_out.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n\nprint(\"✅ submission_tax_prop.tsv created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T19:25:02.729866Z","iopub.execute_input":"2025-11-30T19:25:02.730662Z","iopub.status.idle":"2025-11-30T19:26:04.762933Z","shell.execute_reply.started":"2025-11-30T19:25:02.730627Z","shell.execute_reply":"2025-11-30T19:26:04.762213Z"}},"outputs":[{"name":"stdout","text":"Loading Graph...\nLoading Tax-Aware Predictions...\n","output_type":"stream"},{"name":"stderr","text":"7571663it [00:08, 914039.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Propagating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 224299/224299 [00:48<00:00, 4637.87it/s] ","output_type":"stream"},{"name":"stdout","text":"✅ submission_tax_prop.tsv created!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}