{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88530a83",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-05T02:27:30.000836Z",
     "iopub.status.busy": "2025-12-05T02:27:30.000617Z",
     "iopub.status.idle": "2025-12-05T02:31:14.875803Z",
     "shell.execute_reply": "2025-12-05T02:31:14.874776Z"
    },
    "papermill": {
     "duration": 224.880056,
     "end_time": "2025-12-05T02:31:14.877068",
     "exception": false,
     "start_time": "2025-12-05T02:27:29.997012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading data...\n",
      "Train: (82404, 1280), Test: (224309, 1280)\n",
      "\n",
      "=== TRAINING ===\n",
      "\n",
      ">>> Training Function (F)...\n",
      "Using 2000 GO terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labels: 100%|██████████| 116189/116189 [00:04<00:00, 24309.90it/s]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.0023\n",
      "\n",
      ">>> Training Process (P)...\n",
      "Using 2000 GO terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labels: 100%|██████████| 158939/158939 [00:06<00:00, 24372.26it/s]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.0051\n",
      "\n",
      ">>> Training Component (C)...\n",
      "Using 2000 GO terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labels: 100%|██████████| 156772/156772 [00:06<00:00, 24157.30it/s]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.0033\n",
      "\n",
      " Training Complete!\n",
      "\n",
      "=== PREDICTING ===\n",
      "Predicting F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:03<00:00, 66.57it/s]\n",
      "100%|██████████| 224309/224309 [00:39<00:00, 5706.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting P...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:01<00:00, 129.97it/s]\n",
      "100%|██████████| 224309/224309 [00:37<00:00, 5966.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:01<00:00, 131.80it/s]\n",
      "100%|██████████| 224309/224309 [00:40<00:00, 5486.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Predictions Complete!\n"
     ]
    }
   ],
   "source": [
    "# === V4: OPTIMIZED BASELINE + 2000 TERMS ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"n_terms\": 2000,  # INCREASED from 1500\n",
    "    \"paths\": {\n",
    "        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n",
    "        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n",
    "        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n",
    "        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n",
    "        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n",
    "        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "train_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\n",
    "train_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\n",
    "test_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\n",
    "test_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n",
    "\n",
    "print(f\"Train: {train_emb.shape}, Test: {test_emb.shape}\")\n",
    "\n",
    "# Normalize\n",
    "mean = train_emb.mean(axis=0)\n",
    "std = train_emb.std(axis=0) + 1e-6\n",
    "train_emb = (train_emb - mean) / std\n",
    "test_emb = (test_emb - mean) / std\n",
    "\n",
    "# Load terms\n",
    "terms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n",
    "id_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "# MODEL (Same as your 0.195 winner)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, n_feat, n_class):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_feat, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, n_class)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SimpleData(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y) if y is not None else None\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n",
    "\n",
    "# TRAIN\n",
    "def train_aspect(aspect_char, aspect_name):\n",
    "    print(f\"\\n>>> Training {aspect_name} ({aspect_char})...\")\n",
    "    \n",
    "    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n",
    "    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n",
    "    term_map = {t: i for i, t in enumerate(top_terms)}\n",
    "    num_classes = len(top_terms)\n",
    "    print(f\"Using {num_classes} GO terms\")\n",
    "    \n",
    "    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n",
    "    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n",
    "    \n",
    "    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Labels\"):\n",
    "        if row['id'] in id_to_idx:\n",
    "            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n",
    "    \n",
    "    ds = SimpleData(train_emb, label_matrix)\n",
    "    loader = DataLoader(ds, batch_size=256, shuffle=True, num_workers=2)\n",
    "    \n",
    "    model = SimpleModel(1280, num_classes).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        for x, y in tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total/len(loader):.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"model_v4_{aspect_char}.pth\")\n",
    "    return top_terms\n",
    "\n",
    "# Train all 3\n",
    "print(\"\\n=== TRAINING ===\")\n",
    "results = {}\n",
    "for char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n",
    "    results[char] = train_aspect(char, name)\n",
    "\n",
    "print(\"\\n Training Complete!\")\n",
    "\n",
    "# PREDICT\n",
    "print(\"\\n=== PREDICTING ===\")\n",
    "with open(\"submission_v4_2000terms.tsv\", \"w\") as f:\n",
    "    for char in ['F', 'P', 'C']:\n",
    "        print(f\"Predicting {char}...\")\n",
    "        terms = results[char]\n",
    "        num_classes = len(terms)\n",
    "        \n",
    "        model = SimpleModel(1280, num_classes).to(device)\n",
    "        model.load_state_dict(torch.load(f\"model_v4_{char}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for x in tqdm(loader):\n",
    "                preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n",
    "        \n",
    "        all_preds = np.vstack(preds)\n",
    "        \n",
    "        for i, pid in enumerate(tqdm(test_ids)):\n",
    "            top_idx = np.argpartition(all_preds[i], -70)[-70:]\n",
    "            for idx in top_idx:\n",
    "                if all_preds[i, idx] > 0.01:\n",
    "                    f.write(f\"{pid}\\t{terms[idx]}\\t{all_preds[i, idx]:.3f}\\n\")\n",
    "        \n",
    "        del model, preds\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n Predictions Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06f51bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T02:31:15.034030Z",
     "iopub.status.busy": "2025-12-05T02:31:15.033651Z",
     "iopub.status.idle": "2025-12-05T02:32:46.090813Z",
     "shell.execute_reply": "2025-12-05T02:32:46.089928Z"
    },
    "papermill": {
     "duration": 91.135936,
     "end_time": "2025-12-05T02:32:46.092193",
     "exception": false,
     "start_time": "2025-12-05T02:31:14.956257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GO Graph...\n",
      "Loading predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17114772it [00:15, 1081042.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [01:03<00:00, 3547.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ submission.tsv ready! Download and submit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === GRAPH PROPAGATION ===\n",
    "!pip install obonet networkx -q\n",
    "\n",
    "import obonet\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading GO Graph...\")\n",
    "graph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\n",
    "parent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n",
    "\n",
    "print(\"Loading predictions...\")\n",
    "sub = {}\n",
    "with open(\"submission_v4_2000terms.tsv\") as f:\n",
    "    for line in tqdm(f):\n",
    "        p, t, s = line.strip().split(\"\\t\")\n",
    "        if p not in sub: sub[p] = {}\n",
    "        sub[p][t] = float(s)\n",
    "\n",
    "print(\"Propagating...\")\n",
    "with open(\"submission.tsv\", \"w\") as f:\n",
    "    for pid, preds in tqdm(sub.items()):\n",
    "        final = preds.copy()\n",
    "        q = list(preds.keys())\n",
    "        visited = set(q)\n",
    "        \n",
    "        while q:\n",
    "            term = q.pop(0)\n",
    "            for par in parent_map.get(term, []):\n",
    "                if final.get(par, 0) < final[term]:\n",
    "                    final[par] = final[term]\n",
    "                    if par not in visited:\n",
    "                        q.append(par)\n",
    "                        visited.add(par)\n",
    "        \n",
    "        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n",
    "            if s > 0.001:\n",
    "                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n",
    "\n",
    "print(\"\\n✅ submission.tsv ready! Download and submit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a3d8d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T02:32:46.330282Z",
     "iopub.status.busy": "2025-12-05T02:32:46.329840Z",
     "iopub.status.idle": "2025-12-05T02:37:40.333836Z",
     "shell.execute_reply": "2025-12-05T02:37:40.332906Z"
    },
    "papermill": {
     "duration": 294.135866,
     "end_time": "2025-12-05T02:37:40.335041",
     "exception": false,
     "start_time": "2025-12-05T02:32:46.199175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "\n",
      ">>> Training Function Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112061/112061 [00:04<00:00, 25618.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Model 1/3...\n",
      "    Epoch 1: 0.0174\n",
      "    Epoch 4: 0.0037\n",
      "    Epoch 7: 0.0032\n",
      "    Epoch 10: 0.0029\n",
      "\n",
      "  Model 2/3...\n",
      "    Epoch 1: 0.0174\n",
      "    Epoch 4: 0.0037\n",
      "    Epoch 7: 0.0032\n",
      "    Epoch 10: 0.0029\n",
      "\n",
      "  Model 3/3...\n",
      "    Epoch 1: 0.0172\n",
      "    Epoch 4: 0.0037\n",
      "    Epoch 7: 0.0031\n",
      "    Epoch 10: 0.0029\n",
      "\n",
      ">>> Training Process Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143554/143554 [00:05<00:00, 25453.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Model 1/3...\n",
      "    Epoch 1: 0.0212\n",
      "    Epoch 4: 0.0070\n",
      "    Epoch 7: 0.0064\n",
      "    Epoch 10: 0.0061\n",
      "\n",
      "  Model 2/3...\n",
      "    Epoch 1: 0.0212\n",
      "    Epoch 4: 0.0070\n",
      "    Epoch 7: 0.0064\n",
      "    Epoch 10: 0.0061\n",
      "\n",
      "  Model 3/3...\n",
      "    Epoch 1: 0.0210\n",
      "    Epoch 4: 0.0070\n",
      "    Epoch 7: 0.0064\n",
      "    Epoch 10: 0.0061\n",
      "\n",
      ">>> Training Component Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154977/154977 [00:06<00:00, 25646.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Model 1/3...\n",
      "    Epoch 1: 0.0189\n",
      "    Epoch 4: 0.0050\n",
      "    Epoch 7: 0.0046\n",
      "    Epoch 10: 0.0043\n",
      "\n",
      "  Model 2/3...\n",
      "    Epoch 1: 0.0189\n",
      "    Epoch 4: 0.0050\n",
      "    Epoch 7: 0.0046\n",
      "    Epoch 10: 0.0043\n",
      "\n",
      "  Model 3/3...\n",
      "    Epoch 1: 0.0187\n",
      "    Epoch 4: 0.0050\n",
      "    Epoch 7: 0.0046\n",
      "    Epoch 10: 0.0043\n",
      "\n",
      "=== ENSEMBLE PREDICTION ===\n",
      "\n",
      "Ensembling F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:36<00:00, 6106.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembling P...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:39<00:00, 5610.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembling C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:40<00:00, 5490.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Ensemble complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === ENSEMBLE: 3 MODELS WITH DIFFERENT SEEDS ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"n_terms\": 1500,  # KEEP AT 1500\n",
    "    \"n_models\": 3,     # Train 3 different models\n",
    "    \"paths\": {\n",
    "        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n",
    "        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n",
    "        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n",
    "        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n",
    "        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n",
    "        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# LOAD DATA\n",
    "print(\"Loading...\")\n",
    "train_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\n",
    "train_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\n",
    "test_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\n",
    "test_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n",
    "\n",
    "mean = train_emb.mean(axis=0)\n",
    "std = train_emb.std(axis=0) + 1e-6\n",
    "train_emb = (train_emb - mean) / std\n",
    "test_emb = (test_emb - mean) / std\n",
    "\n",
    "terms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n",
    "id_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "# MODEL\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, n_feat, n_class):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_feat, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, n_class)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SimpleData(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y) if y is not None else None\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n",
    "\n",
    "# TRAIN MULTIPLE MODELS\n",
    "def train_ensemble(aspect_char, aspect_name):\n",
    "    print(f\"\\n>>> Training {aspect_name} Ensemble...\")\n",
    "    \n",
    "    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n",
    "    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n",
    "    term_map = {t: i for i, t in enumerate(top_terms)}\n",
    "    \n",
    "    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n",
    "    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n",
    "    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n",
    "        if row['id'] in id_to_idx:\n",
    "            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n",
    "    \n",
    "    # Train N models with different seeds\n",
    "    for model_idx in range(CONFIG[\"n_models\"]):\n",
    "        print(f\"\\n  Model {model_idx+1}/{CONFIG['n_models']}...\")\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(42 + model_idx)\n",
    "        np.random.seed(42 + model_idx)\n",
    "        \n",
    "        ds = SimpleData(train_emb, label_matrix)\n",
    "        loader = DataLoader(ds, batch_size=256, shuffle=True)\n",
    "        \n",
    "        model = SimpleModel(1280, len(top_terms)).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(10):\n",
    "            model.train()\n",
    "            total = 0\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(model(x), y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total += loss.item()\n",
    "            if epoch % 3 == 0:\n",
    "                print(f\"    Epoch {epoch+1}: {total/len(loader):.4f}\")\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"model_ens_{aspect_char}_{model_idx}.pth\")\n",
    "    \n",
    "    return top_terms\n",
    "\n",
    "# Train ensembles\n",
    "results = {}\n",
    "for char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n",
    "    results[char] = train_ensemble(char, name)\n",
    "\n",
    "# PREDICT (Average all models)\n",
    "print(\"\\n=== ENSEMBLE PREDICTION ===\")\n",
    "with open(\"submission_ensemble.tsv\", \"w\") as f:\n",
    "    for char in ['F', 'P', 'C']:\n",
    "        print(f\"\\nEnsembling {char}...\")\n",
    "        terms = results[char]\n",
    "        \n",
    "        # Collect predictions from all models\n",
    "        all_model_preds = []\n",
    "        for model_idx in range(CONFIG[\"n_models\"]):\n",
    "            model = SimpleModel(1280, len(terms)).to(device)\n",
    "            model.load_state_dict(torch.load(f\"model_ens_{char}_{model_idx}.pth\"))\n",
    "            model.eval()\n",
    "            \n",
    "            loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                for x in loader:\n",
    "                    preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n",
    "            \n",
    "            all_model_preds.append(np.vstack(preds))\n",
    "            del model\n",
    "        \n",
    "        # AVERAGE predictions\n",
    "        final_preds = np.mean(all_model_preds, axis=0)\n",
    "        \n",
    "        for i, pid in enumerate(tqdm(test_ids)):\n",
    "            top_idx = np.argpartition(final_preds[i], -70)[-70:]\n",
    "            for idx in top_idx:\n",
    "                if final_preds[i, idx] > 0.01:\n",
    "                    f.write(f\"{pid}\\t{terms[idx]}\\t{final_preds[i, idx]:.3f}\\n\")\n",
    "        \n",
    "        del all_model_preds\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n✅ Ensemble complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7eff1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T02:37:40.653177Z",
     "iopub.status.busy": "2025-12-05T02:37:40.652508Z",
     "iopub.status.idle": "2025-12-05T02:39:11.903441Z",
     "shell.execute_reply": "2025-12-05T02:39:11.902577Z"
    },
    "papermill": {
     "duration": 91.411038,
     "end_time": "2025-12-05T02:39:11.904602",
     "exception": false,
     "start_time": "2025-12-05T02:37:40.493564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [01:08<00:00, 3268.21it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install obonet -q\n",
    "import obonet\n",
    "graph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\n",
    "parent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n",
    "\n",
    "sub = {}\n",
    "with open(\"submission_ensemble.tsv\") as f:\n",
    "    for line in f:\n",
    "        p, t, s = line.strip().split(\"\\t\")\n",
    "        if p not in sub: sub[p] = {}\n",
    "        sub[p][t] = float(s)\n",
    "\n",
    "with open(\"submissionfinal.tsv\", \"w\") as f:\n",
    "    for pid, preds in tqdm(sub.items()):\n",
    "        final = preds.copy()\n",
    "        q = list(preds.keys())\n",
    "        while q:\n",
    "            term = q.pop(0)\n",
    "            for par in parent_map.get(term, []):\n",
    "                if final.get(par, 0) < final[term]:\n",
    "                    final[par] = final[term]\n",
    "                    if par not in preds: q.append(par)\n",
    "        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n",
    "            if s > 0.001:\n",
    "                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bfe43d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T02:39:12.271589Z",
     "iopub.status.busy": "2025-12-05T02:39:12.271305Z",
     "iopub.status.idle": "2025-12-05T02:44:52.505939Z",
     "shell.execute_reply": "2025-12-05T02:44:52.504712Z"
    },
    "papermill": {
     "duration": 340.417182,
     "end_time": "2025-12-05T02:44:52.507204",
     "exception": false,
     "start_time": "2025-12-05T02:39:12.090022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "\n",
      ">>> Training Function Diverse Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112061/112061 [00:04<00:00, 25478.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training Simple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0029\n",
      "\n",
      "  Training Deep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0030\n",
      "\n",
      "  Training Wide+BN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0029\n",
      "\n",
      ">>> Training Process Diverse Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143554/143554 [00:05<00:00, 25693.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training Simple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0061\n",
      "\n",
      "  Training Deep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0064\n",
      "\n",
      "  Training Wide+BN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0061\n",
      "\n",
      ">>> Training Component Diverse Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154977/154977 [00:06<00:00, 25720.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training Simple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0043\n",
      "\n",
      "  Training Deep...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0044\n",
      "\n",
      "  Training Wide+BN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10: 0.0043\n",
      "\n",
      "=== DIVERSE ENSEMBLE PREDICTION ===\n",
      "\n",
      "Ensembling F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:37<00:00, 6009.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembling P...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:40<00:00, 5497.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensembling C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [00:41<00:00, 5459.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done!\n"
     ]
    }
   ],
   "source": [
    "# === DIVERSE ARCHITECTURE ENSEMBLE ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"n_terms\": 1500,\n",
    "    \"paths\": {\n",
    "        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n",
    "        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n",
    "        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n",
    "        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n",
    "        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n",
    "        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# LOAD DATA\n",
    "print(\"Loading...\")\n",
    "train_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\n",
    "train_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\n",
    "test_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\n",
    "test_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n",
    "\n",
    "mean = train_emb.mean(axis=0)\n",
    "std = train_emb.std(axis=0) + 1e-6\n",
    "train_emb = (train_emb - mean) / std\n",
    "test_emb = (test_emb - mean) / std\n",
    "\n",
    "terms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\n",
    "id_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n",
    "\n",
    "# THREE DIFFERENT ARCHITECTURES\n",
    "class Model_A(nn.Module):  # Original (2-layer, 512)\n",
    "    def __init__(self, n_feat, n_class):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_feat, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, n_class)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Model_B(nn.Module):  # Deeper (3-layer)\n",
    "    def __init__(self, n_feat, n_class):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_feat, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, n_class)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Model_C(nn.Module):  # Wider + BatchNorm\n",
    "    def __init__(self, n_feat, n_class):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_feat, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, n_class)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SimpleData(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y) if y is not None else None\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n",
    "\n",
    "# TRAIN\n",
    "def train_diverse_ensemble(aspect_char, aspect_name):\n",
    "    print(f\"\\n>>> Training {aspect_name} Diverse Ensemble...\")\n",
    "    \n",
    "    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n",
    "    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n",
    "    term_map = {t: i for i, t in enumerate(top_terms)}\n",
    "    \n",
    "    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n",
    "    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n",
    "    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n",
    "        if row['id'] in id_to_idx:\n",
    "            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n",
    "    \n",
    "    models = [Model_A, Model_B, Model_C]\n",
    "    model_names = ['Simple', 'Deep', 'Wide+BN']\n",
    "    \n",
    "    for idx, (ModelClass, name) in enumerate(zip(models, model_names)):\n",
    "        print(f\"\\n  Training {name}...\")\n",
    "        torch.manual_seed(42 + idx)\n",
    "        \n",
    "        ds = SimpleData(train_emb, label_matrix)\n",
    "        loader = DataLoader(ds, batch_size=256, shuffle=True, num_workers=2)\n",
    "        \n",
    "        model = ModelClass(1280, len(top_terms)).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(10):\n",
    "            model.train()\n",
    "            total = 0\n",
    "            for x, y in tqdm(loader, leave=False):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(model(x), y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total += loss.item()\n",
    "            if epoch % 3 == 0:\n",
    "                print(f\"    Epoch {epoch+1}: {total/len(loader):.4f}\")\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"model_diverse_{aspect_char}_{idx}.pth\")\n",
    "    \n",
    "    return top_terms\n",
    "\n",
    "# Train\n",
    "results = {}\n",
    "for char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n",
    "    results[char] = train_diverse_ensemble(char, name)\n",
    "\n",
    "# PREDICT\n",
    "print(\"\\n=== DIVERSE ENSEMBLE PREDICTION ===\")\n",
    "with open(\"submission_diverse.tsv\", \"w\") as f:\n",
    "    for char in ['F', 'P', 'C']:\n",
    "        print(f\"\\nEnsembling {char}...\")\n",
    "        terms = results[char]\n",
    "        models = [Model_A, Model_B, Model_C]\n",
    "        \n",
    "        all_preds = []\n",
    "        for idx, ModelClass in enumerate(models):\n",
    "            model = ModelClass(1280, len(terms)).to(device)\n",
    "            model.load_state_dict(torch.load(f\"model_diverse_{char}_{idx}.pth\"))\n",
    "            model.eval()\n",
    "            \n",
    "            loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                for x in tqdm(loader, leave=False):\n",
    "                    preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n",
    "            \n",
    "            all_preds.append(np.vstack(preds))\n",
    "            del model\n",
    "        \n",
    "        # Average\n",
    "        final_preds = np.mean(all_preds, axis=0)\n",
    "        \n",
    "        for i, pid in enumerate(tqdm(test_ids)):\n",
    "            top_idx = np.argpartition(final_preds[i], -70)[-70:]\n",
    "            for idx in top_idx:\n",
    "                if final_preds[i, idx] > 0.01:\n",
    "                    f.write(f\"{pid}\\t{terms[idx]}\\t{final_preds[i, idx]:.3f}\\n\")\n",
    "        \n",
    "        del all_preds\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n✅ Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a36929b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T02:44:53.213270Z",
     "iopub.status.busy": "2025-12-05T02:44:53.212490Z",
     "iopub.status.idle": "2025-12-05T02:46:39.003893Z",
     "shell.execute_reply": "2025-12-05T02:46:39.003044Z"
    },
    "papermill": {
     "duration": 106.106791,
     "end_time": "2025-12-05T02:46:39.005224",
     "exception": false,
     "start_time": "2025-12-05T02:44:52.898433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224309/224309 [01:21<00:00, 2761.17it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install obonet -q\n",
    "import obonet\n",
    "graph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\n",
    "parent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n",
    "\n",
    "sub = {}\n",
    "with open(\"submission_diverse.tsv\") as f:\n",
    "    for line in f:\n",
    "        p, t, s = line.strip().split(\"\\t\")\n",
    "        if p not in sub: sub[p] = {}\n",
    "        sub[p][t] = float(s)\n",
    "\n",
    "with open(\"submissionSimple+Deep+Wide.tsv\", \"w\") as f:\n",
    "    for pid, preds in tqdm(sub.items()):\n",
    "        final = preds.copy()\n",
    "        q = list(preds.keys())\n",
    "        while q:\n",
    "            term = q.pop(0)\n",
    "            for par in parent_map.get(term, []):\n",
    "                if final.get(par, 0) < final[term]:\n",
    "                    final[par] = final[term]\n",
    "                    if par not in preds: q.append(par)\n",
    "        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n",
    "            if s > 0.001:\n",
    "                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f81735",
   "metadata": {
    "papermill": {
     "duration": 0.415172,
     "end_time": "2025-12-05T02:46:39.763890",
     "exception": false,
     "start_time": "2025-12-05T02:46:39.348718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14084779,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8921182,
     "sourceId": 14000427,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 282785747,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 283963830,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1158.098496,
   "end_time": "2025-12-05T02:46:43.253891",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-05T02:27:25.155395",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
