{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"},{"sourceId":14000427,"sourceType":"datasetVersion","datasetId":8921182},{"sourceId":282785747,"sourceType":"kernelVersion"},{"sourceId":283963830,"sourceType":"kernelVersion"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === V4: OPTIMIZED BASELINE + 2000 TERMS ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nCONFIG = {\n    \"n_terms\": 2000,  # INCREASED from 1500\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# LOAD DATA\nprint(\"Loading data...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nprint(f\"Train: {train_emb.shape}, Test: {test_emb.shape}\")\n\n# Normalize\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\n# Load terms\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# MODEL (Same as your 0.195 winner)\nclass SimpleModel(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleData(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n\n# TRAIN\ndef train_aspect(aspect_char, aspect_name):\n    print(f\"\\n>>> Training {aspect_name} ({aspect_char})...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    num_classes = len(top_terms)\n    print(f\"Using {num_classes} GO terms\")\n    \n    label_matrix = np.zeros((len(train_ids), num_classes), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Labels\"):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    ds = SimpleData(train_emb, label_matrix)\n    loader = DataLoader(ds, batch_size=256, shuffle=True, num_workers=2)\n    \n    model = SimpleModel(1280, num_classes).to(device)\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    for epoch in range(10):\n        model.train()\n        total = 0\n        for x, y in tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False):\n            x, y = x.to(device), y.to(device)\n            opt.zero_grad()\n            loss = loss_fn(model(x), y)\n            loss.backward()\n            opt.step()\n            total += loss.item()\n        print(f\"Epoch {epoch+1} Loss: {total/len(loader):.4f}\")\n    \n    torch.save(model.state_dict(), f\"model_v4_{aspect_char}.pth\")\n    return top_terms\n\n# Train all 3\nprint(\"\\n=== TRAINING ===\")\nresults = {}\nfor char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    results[char] = train_aspect(char, name)\n\nprint(\"\\n Training Complete!\")\n\n# PREDICT\nprint(\"\\n=== PREDICTING ===\")\nwith open(\"submission_v4_2000terms.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        print(f\"Predicting {char}...\")\n        terms = results[char]\n        num_classes = len(terms)\n        \n        model = SimpleModel(1280, num_classes).to(device)\n        model.load_state_dict(torch.load(f\"model_v4_{char}.pth\"))\n        model.eval()\n        \n        loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n        preds = []\n        with torch.no_grad():\n            for x in tqdm(loader):\n                preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n        \n        all_preds = np.vstack(preds)\n        \n        for i, pid in enumerate(tqdm(test_ids)):\n            top_idx = np.argpartition(all_preds[i], -70)[-70:]\n            for idx in top_idx:\n                if all_preds[i, idx] > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{all_preds[i, idx]:.3f}\\n\")\n        \n        del model, preds\n        gc.collect()\n\nprint(\"\\n Predictions Complete!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T01:38:30.107946Z","iopub.execute_input":"2025-12-05T01:38:30.108135Z","iopub.status.idle":"2025-12-05T01:42:19.217409Z","shell.execute_reply.started":"2025-12-05T01:38:30.108117Z","shell.execute_reply":"2025-12-05T01:42:19.216506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === GRAPH PROPAGATION ===\n!pip install obonet networkx -q\n\nimport obonet\nfrom tqdm import tqdm\n\nprint(\"Loading GO Graph...\")\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\nprint(\"Loading predictions...\")\nsub = {}\nwith open(\"submission_v4_2000terms.tsv\") as f:\n    for line in tqdm(f):\n        p, t, s = line.strip().split(\"\\t\")\n        if p not in sub: sub[p] = {}\n        sub[p][t] = float(s)\n\nprint(\"Propagating...\")\nwith open(\"submission.tsv\", \"w\") as f:\n    for pid, preds in tqdm(sub.items()):\n        final = preds.copy()\n        q = list(preds.keys())\n        visited = set(q)\n        \n        while q:\n            term = q.pop(0)\n            for par in parent_map.get(term, []):\n                if final.get(par, 0) < final[term]:\n                    final[par] = final[term]\n                    if par not in visited:\n                        q.append(par)\n                        visited.add(par)\n        \n        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n            if s > 0.001:\n                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n\nprint(\"\\n✅ submission.tsv ready! Download and submit.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T01:42:34.833764Z","iopub.execute_input":"2025-12-05T01:42:34.834366Z","iopub.status.idle":"2025-12-05T01:43:58.687306Z","shell.execute_reply.started":"2025-12-05T01:42:34.834335Z","shell.execute_reply":"2025-12-05T01:43:58.686622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === ENSEMBLE: 3 MODELS WITH DIFFERENT SEEDS ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCONFIG = {\n    \"n_terms\": 1500,  # KEEP AT 1500\n    \"n_models\": 3,     # Train 3 different models\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# LOAD DATA\nprint(\"Loading...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# MODEL\nclass SimpleModel(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleData(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n\n# TRAIN MULTIPLE MODELS\ndef train_ensemble(aspect_char, aspect_name):\n    print(f\"\\n>>> Training {aspect_name} Ensemble...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    # Train N models with different seeds\n    for model_idx in range(CONFIG[\"n_models\"]):\n        print(f\"\\n  Model {model_idx+1}/{CONFIG['n_models']}...\")\n        \n        # Set seed for reproducibility\n        torch.manual_seed(42 + model_idx)\n        np.random.seed(42 + model_idx)\n        \n        ds = SimpleData(train_emb, label_matrix)\n        loader = DataLoader(ds, batch_size=256, shuffle=True)\n        \n        model = SimpleModel(1280, len(top_terms)).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        for epoch in range(10):\n            model.train()\n            total = 0\n            for x, y in loader:\n                x, y = x.to(device), y.to(device)\n                opt.zero_grad()\n                loss = loss_fn(model(x), y)\n                loss.backward()\n                opt.step()\n                total += loss.item()\n            if epoch % 3 == 0:\n                print(f\"    Epoch {epoch+1}: {total/len(loader):.4f}\")\n        \n        torch.save(model.state_dict(), f\"model_ens_{aspect_char}_{model_idx}.pth\")\n    \n    return top_terms\n\n# Train ensembles\nresults = {}\nfor char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    results[char] = train_ensemble(char, name)\n\n# PREDICT (Average all models)\nprint(\"\\n=== ENSEMBLE PREDICTION ===\")\nwith open(\"submission_ensemble.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        print(f\"\\nEnsembling {char}...\")\n        terms = results[char]\n        \n        # Collect predictions from all models\n        all_model_preds = []\n        for model_idx in range(CONFIG[\"n_models\"]):\n            model = SimpleModel(1280, len(terms)).to(device)\n            model.load_state_dict(torch.load(f\"model_ens_{char}_{model_idx}.pth\"))\n            model.eval()\n            \n            loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n            preds = []\n            with torch.no_grad():\n                for x in loader:\n                    preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n            \n            all_model_preds.append(np.vstack(preds))\n            del model\n        \n        # AVERAGE predictions\n        final_preds = np.mean(all_model_preds, axis=0)\n        \n        for i, pid in enumerate(tqdm(test_ids)):\n            top_idx = np.argpartition(final_preds[i], -70)[-70:]\n            for idx in top_idx:\n                if final_preds[i, idx] > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{final_preds[i, idx]:.3f}\\n\")\n        \n        del all_model_preds\n        gc.collect()\n\nprint(\"\\n✅ Ensemble complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T01:55:05.638037Z","iopub.execute_input":"2025-12-05T01:55:05.638707Z","iopub.status.idle":"2025-12-05T02:00:24.114547Z","shell.execute_reply.started":"2025-12-05T01:55:05.638675Z","shell.execute_reply":"2025-12-05T02:00:24.113834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install obonet -q\nimport obonet\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\nsub = {}\nwith open(\"submission_ensemble.tsv\") as f:\n    for line in f:\n        p, t, s = line.strip().split(\"\\t\")\n        if p not in sub: sub[p] = {}\n        sub[p][t] = float(s)\n\nwith open(\"submissionfinal.tsv\", \"w\") as f:\n    for pid, preds in tqdm(sub.items()):\n        final = preds.copy()\n        q = list(preds.keys())\n        while q:\n            term = q.pop(0)\n            for par in parent_map.get(term, []):\n                if final.get(par, 0) < final[term]:\n                    final[par] = final[term]\n                    if par not in preds: q.append(par)\n        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n            if s > 0.001:\n                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T02:00:38.237829Z","iopub.execute_input":"2025-12-05T02:00:38.238441Z","iopub.status.idle":"2025-12-05T02:02:19.578023Z","shell.execute_reply.started":"2025-12-05T02:00:38.238416Z","shell.execute_reply":"2025-12-05T02:02:19.577339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === DIVERSE ARCHITECTURE ENSEMBLE ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCONFIG = {\n    \"n_terms\": 1500,\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# LOAD DATA\nprint(\"Loading...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# THREE DIFFERENT ARCHITECTURES\nclass Model_A(nn.Module):  # Original (2-layer, 512)\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass Model_B(nn.Module):  # Deeper (3-layer)\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass Model_C(nn.Module):  # Wider + BatchNorm\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleData(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n\n# TRAIN\ndef train_diverse_ensemble(aspect_char, aspect_name):\n    print(f\"\\n>>> Training {aspect_name} Diverse Ensemble...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    models = [Model_A, Model_B, Model_C]\n    model_names = ['Simple', 'Deep', 'Wide+BN']\n    \n    for idx, (ModelClass, name) in enumerate(zip(models, model_names)):\n        print(f\"\\n  Training {name}...\")\n        torch.manual_seed(42 + idx)\n        \n        ds = SimpleData(train_emb, label_matrix)\n        loader = DataLoader(ds, batch_size=256, shuffle=True, num_workers=2)\n        \n        model = ModelClass(1280, len(top_terms)).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        for epoch in range(10):\n            model.train()\n            total = 0\n            for x, y in tqdm(loader, leave=False):\n                x, y = x.to(device), y.to(device)\n                opt.zero_grad()\n                loss = loss_fn(model(x), y)\n                loss.backward()\n                opt.step()\n                total += loss.item()\n            if epoch % 3 == 0:\n                print(f\"    Epoch {epoch+1}: {total/len(loader):.4f}\")\n        \n        torch.save(model.state_dict(), f\"model_diverse_{aspect_char}_{idx}.pth\")\n    \n    return top_terms\n\n# Train\nresults = {}\nfor char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    results[char] = train_diverse_ensemble(char, name)\n\n# PREDICT\nprint(\"\\n=== DIVERSE ENSEMBLE PREDICTION ===\")\nwith open(\"submission_diverse.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        print(f\"\\nEnsembling {char}...\")\n        terms = results[char]\n        models = [Model_A, Model_B, Model_C]\n        \n        all_preds = []\n        for idx, ModelClass in enumerate(models):\n            model = ModelClass(1280, len(terms)).to(device)\n            model.load_state_dict(torch.load(f\"model_diverse_{char}_{idx}.pth\"))\n            model.eval()\n            \n            loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n            preds = []\n            with torch.no_grad():\n                for x in tqdm(loader, leave=False):\n                    preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n            \n            all_preds.append(np.vstack(preds))\n            del model\n        \n        # Average\n        final_preds = np.mean(all_preds, axis=0)\n        \n        for i, pid in enumerate(tqdm(test_ids)):\n            top_idx = np.argpartition(final_preds[i], -70)[-70:]\n            for idx in top_idx:\n                if final_preds[i, idx] > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{final_preds[i, idx]:.3f}\\n\")\n        \n        del all_preds\n        gc.collect()\n\nprint(\"\\n✅ Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T02:13:02.321419Z","iopub.execute_input":"2025-12-05T02:13:02.321800Z","iopub.status.idle":"2025-12-05T02:19:12.392704Z","shell.execute_reply.started":"2025-12-05T02:13:02.321768Z","shell.execute_reply":"2025-12-05T02:19:12.391878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install obonet -q\nimport obonet\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\nsub = {}\nwith open(\"submission_diverse.tsv\") as f:\n    for line in f:\n        p, t, s = line.strip().split(\"\\t\")\n        if p not in sub: sub[p] = {}\n        sub[p][t] = float(s)\n\nwith open(\"submissionSimple+Deep+Wide.tsv\", \"w\") as f:\n    for pid, preds in tqdm(sub.items()):\n        final = preds.copy()\n        q = list(preds.keys())\n        while q:\n            term = q.pop(0)\n            for par in parent_map.get(term, []):\n                if final.get(par, 0) < final[term]:\n                    final[par] = final[term]\n                    if par not in preds: q.append(par)\n        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n            if s > 0.001:\n                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T02:20:10.749727Z","iopub.execute_input":"2025-12-05T02:20:10.750574Z","iopub.status.idle":"2025-12-05T02:22:07.219747Z","shell.execute_reply.started":"2025-12-05T02:20:10.750537Z","shell.execute_reply":"2025-12-05T02:22:07.218620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === 5-MODEL ENSEMBLE (Conservative) ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCONFIG = {\n    \"n_terms\": 1500,\n    \"n_models\": 5,  # INCREASED from 3\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# LOAD DATA\nprint(\"Loading...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# SIMPLE MODEL (What got you 0.209)\nclass SimpleModel(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleData(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n\n# TRAIN\ndef train_ensemble(aspect_char, aspect_name):\n    print(f\"\\n>>> {aspect_name} - Training {CONFIG['n_models']} models...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    for _, row in tqdm(relevant.iterrows(), total=len(relevant), desc=\"Labels\"):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    for model_idx in range(CONFIG[\"n_models\"]):\n        print(f\"\\n  Model {model_idx+1}/{CONFIG['n_models']}...\")\n        torch.manual_seed(42 + model_idx * 10)\n        np.random.seed(42 + model_idx * 10)\n        \n        ds = SimpleData(train_emb, label_matrix)\n        loader = DataLoader(ds, batch_size=256, shuffle=True, num_workers=2)\n        \n        model = SimpleModel(1280, len(top_terms)).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        for epoch in range(10):\n            model.train()\n            total = 0\n            for x, y in tqdm(loader, leave=False):\n                x, y = x.to(device), y.to(device)\n                opt.zero_grad()\n                loss = loss_fn(model(x), y)\n                loss.backward()\n                opt.step()\n                total += loss.item()\n            if epoch % 3 == 0:\n                print(f\"    Epoch {epoch+1}: {total/len(loader):.4f}\")\n        \n        torch.save(model.state_dict(), f\"model_5ens_{aspect_char}_{model_idx}.pth\")\n    \n    return top_terms\n\n# Train\nresults = {}\nfor char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    results[char] = train_ensemble(char, name)\n\n# PREDICT\nprint(\"\\n=== 5-MODEL ENSEMBLE PREDICTION ===\")\nwith open(\"submission_5ens.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        print(f\"\\nEnsembling {char}...\")\n        terms = results[char]\n        \n        all_preds = []\n        for model_idx in range(CONFIG[\"n_models\"]):\n            model = SimpleModel(1280, len(terms)).to(device)\n            model.load_state_dict(torch.load(f\"model_5ens_{char}_{model_idx}.pth\"))\n            model.eval()\n            \n            loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n            preds = []\n            with torch.no_grad():\n                for x in tqdm(loader, leave=False):\n                    preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n            \n            all_preds.append(np.vstack(preds))\n            del model\n        \n        # Average all 5\n        final_preds = np.mean(all_preds, axis=0)\n        \n        for i, pid in enumerate(tqdm(test_ids)):\n            top_idx = np.argpartition(final_preds[i], -70)[-70:]\n            for idx in top_idx:\n                if final_preds[i, idx] > 0.01:\n                    f.write(f\"{pid}\\t{terms[idx]}\\t{final_preds[i, idx]:.3f}\\n\")\n        \n        del all_preds\n        gc.collect()\n\nprint(\"\\n✅ Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:00:14.219840Z","iopub.execute_input":"2025-12-06T15:00:14.220021Z","iopub.status.idle":"2025-12-06T15:07:35.559199Z","shell.execute_reply.started":"2025-12-06T15:00:14.220005Z","shell.execute_reply":"2025-12-06T15:07:35.558518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install obonet -q\nimport obonet\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\nsub = {}\nwith open(\"submission_5ens.tsv\") as f:\n    for line in f:\n        p, t, s = line.strip().split(\"\\t\")\n        if p not in sub: sub[p] = {}\n        sub[p][t] = float(s)\n\nwith open(\"submissionenvdd.tsv\", \"w\") as f:\n    for pid, preds in tqdm(sub.items()):\n        final = preds.copy()\n        q = list(preds.keys())\n        while q:\n            term = q.pop(0)\n            for par in parent_map.get(term, []):\n                if final.get(par, 0) < final[term]:\n                    final[par] = final[term]\n                    if par not in preds: q.append(par)\n        for t, s in sorted(final.items(), key=lambda x: -x[1])[:70]:\n            if s > 0.001:\n                f.write(f\"{pid}\\t{t}\\t{s:.3f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:09:47.123937Z","iopub.execute_input":"2025-12-06T15:09:47.124232Z","iopub.status.idle":"2025-12-06T15:11:05.176583Z","shell.execute_reply.started":"2025-12-06T15:09:47.124211Z","shell.execute_reply":"2025-12-06T15:11:05.175919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === BASELINE: 3-MODEL ENSEMBLE ===\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nCONFIG = {\n    \"n_terms\": 1500,\n    \"n_models\": 3,\n    \"paths\": {\n        \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n        \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n        \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n        \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n        \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n        \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n    }\n}\n\n# LOAD DATA\nprint(\"Loading...\")\ntrain_emb = np.load(CONFIG[\"paths\"][\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(CONFIG[\"paths\"][\"train_ids\"])\ntest_emb = np.load(CONFIG[\"paths\"][\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(CONFIG[\"paths\"][\"test_ids\"])\n\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nterms_df = pd.read_csv(CONFIG[\"paths\"][\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\n# MODEL\nclass SimpleModel(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    def forward(self, x): return self.net(x)\n\nclass SimpleData(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n\n# TRAIN\ndef train_ensemble(aspect_char, aspect_name):\n    print(f\"\\n>>> {aspect_name}...\")\n    \n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:CONFIG[\"n_terms\"]].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    for _, row in tqdm(relevant.iterrows(), total=len(relevant)):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    for model_idx in range(CONFIG[\"n_models\"]):\n        print(f\"  Training model {model_idx+1}/3...\")\n        torch.manual_seed(42 + model_idx)\n        \n        ds = SimpleData(train_emb, label_matrix)\n        loader = DataLoader(ds, batch_size=256, shuffle=True, num_workers=2)\n        \n        model = SimpleModel(1280, len(top_terms)).to(device)\n        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n        loss_fn = nn.BCEWithLogitsLoss()\n        \n        for epoch in range(10):\n            model.train()\n            total = 0\n            for x, y in tqdm(loader, leave=False):\n                x, y = x.to(device), y.to(device)\n                opt.zero_grad()\n                loss = loss_fn(model(x), y)\n                loss.backward()\n                opt.step()\n                total += loss.item()\n        \n        torch.save(model.state_dict(), f\"model_{aspect_char}_{model_idx}.pth\")\n    \n    return top_terms\n\n# Train all aspects\nresults = {}\nfor char, name in [('F', 'Function'), ('P', 'Process'), ('C', 'Component')]:\n    results[char] = train_ensemble(char, name)\n\n# GENERATE ENSEMBLE PREDICTIONS (Save to dictionary)\nprint(\"\\n=== Generating Base Predictions ===\")\npredictions = {}  # {aspect: {protein_id: {term: score}}}\n\nfor char in ['F', 'P', 'C']:\n    print(f\"\\nEnsembling {char}...\")\n    terms = results[char]\n    \n    all_preds = []\n    for model_idx in range(CONFIG[\"n_models\"]):\n        model = SimpleModel(1280, len(terms)).to(device)\n        model.load_state_dict(torch.load(f\"model_{char}_{model_idx}.pth\"))\n        model.eval()\n        \n        loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n        preds = []\n        with torch.no_grad():\n            for x in tqdm(loader, leave=False):\n                preds.append(torch.sigmoid(model(x.to(device))).cpu().numpy())\n        \n        all_preds.append(np.vstack(preds))\n        del model\n    \n    # Average\n    final_preds = np.mean(all_preds, axis=0)\n    \n    # Store in dictionary\n    predictions[char] = {}\n    for i, pid in enumerate(test_ids):\n        predictions[char][pid] = {terms[j]: final_preds[i, j] for j in range(len(terms))}\n    \n    del all_preds\n    gc.collect()\n\nprint(\"\\n✅ Base predictions ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T15:22:02.866291Z","iopub.execute_input":"2025-12-06T15:22:02.866617Z","execution_failed":"2025-12-06T15:38:18.324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === SYSTEMATIC POST-PROCESSING OPTIMIZATION ===\n!pip install obonet networkx -q\n\nimport obonet\nfrom itertools import product\n\n# Load GO graph\nprint(\"Loading GO graph...\")\ngraph = obonet.read_obo(CONFIG[\"paths\"][\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\n# Test configurations\nthresholds = [0.005, 0.008, 0.01, 0.015]\ntopk_before = [60, 70, 80]\ntopk_after = [70, 80, 100]\n\nprint(f\"\\n=== Testing {len(thresholds) * len(topk_before) * len(topk_after)} variants ===\\n\")\n\nvariant_count = 0\nfor thresh, k_before, k_after in product(thresholds, topk_before, topk_after):\n    variant_count += 1\n    filename = f\"sub_t{int(thresh*1000)}_kb{k_before}_ka{k_after}.tsv\"\n    \n    print(f\"[{variant_count}] thresh={thresh}, topk_pre={k_before}, topk_post={k_after}\")\n    \n    with open(filename, \"w\") as f:\n        for char in ['F', 'P', 'C']:\n            terms = results[char]\n            \n            for pid in tqdm(test_ids, leave=False):\n                pred_dict = predictions[char][pid]\n                \n                # Filter by threshold and select top-k BEFORE propagation\n                filtered = {t: s for t, s in pred_dict.items() if s > thresh}\n                if len(filtered) > k_before:\n                    sorted_items = sorted(filtered.items(), key=lambda x: -x[1])\n                    filtered = dict(sorted_items[:k_before])\n                \n                # Propagate\n                final_scores = filtered.copy()\n                queue = list(filtered.keys())\n                visited = set(queue)\n                \n                while queue:\n                    term = queue.pop(0)\n                    score = final_scores.get(term, 0.0)\n                    if term in parent_map:\n                        for parent in parent_map[term]:\n                            old = final_scores.get(parent, 0.0)\n                            if score > old:\n                                final_scores[parent] = score\n                                if parent not in visited:\n                                    queue.append(parent)\n                                    visited.add(parent)\n                \n                # Select top-k AFTER propagation\n                sorted_final = sorted(final_scores.items(), key=lambda x: -x[1])[:k_after]\n                \n                for term, score in sorted_final:\n                    if score > 0.001:\n                        f.write(f\"{pid}\\t{term}\\t{score:.3f}\\n\")\n    \n    print(f\"  ✓ Saved to {filename}\")\n\nprint(f\"\\n✅ Generated {variant_count} submission variants!\")\nprint(\"\\nNow submit each file and track which performs best.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# PSEUDO-LABELING: 0.209 → 0.25+ Expected\n# Uses your existing embeddings + models\n# ============================================\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# Paths\nPATHS = {\n    \"train_emb\": \"/kaggle/input/esm-dataset/train_embeds.npy\",\n    \"train_ids\": \"/kaggle/input/esm-dataset/train_ids.npy\",\n    \"test_emb\": \"/kaggle/input/esm-dataset/test_embeds.npy\",\n    \"test_ids\": \"/kaggle/input/esm-dataset/test_ids.npy\",\n    \"train_terms\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\",\n    \"go_obo\": \"/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\"\n}\n\n# Load data\nprint(\"Loading embeddings...\")\ntrain_emb = np.load(PATHS[\"train_emb\"]).astype(np.float32)\ntrain_ids = np.load(PATHS[\"train_ids\"])\ntest_emb = np.load(PATHS[\"test_emb\"]).astype(np.float32)\ntest_ids = np.load(PATHS[\"test_ids\"])\n\n# Normalize\nmean = train_emb.mean(axis=0)\nstd = train_emb.std(axis=0) + 1e-6\ntrain_emb = (train_emb - mean) / std\ntest_emb = (test_emb - mean) / std\n\nprint(f\"✓ Train: {train_emb.shape}\")\nprint(f\"✓ Test: {test_emb.shape}\")\n\n# Load GO terms\nterms_df = pd.read_csv(PATHS[\"train_terms\"], sep=\"\\t\", header=None, names=[\"id\", \"term\", \"aspect\"])\nid_to_idx = {pid: i for i, pid in enumerate(train_ids)}\n\nprint(f\"✓ GO annotations: {len(terms_df):,} rows\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:10:43.525099Z","iopub.execute_input":"2025-12-10T23:10:43.525298Z","iopub.status.idle":"2025-12-10T23:11:01.673230Z","shell.execute_reply.started":"2025-12-10T23:10:43.525280Z","shell.execute_reply":"2025-12-10T23:11:01.672414Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoading embeddings...\n✓ Train: (82404, 1280)\n✓ Test: (224309, 1280)\n✓ GO annotations: 537,028 rows\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================\n# MODEL ARCHITECTURE\n# ============================================\n\nclass SimpleModel(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_feat, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, n_class)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\nclass SimpleDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y) if y is not None else None\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\nprint(\"✓ Model architecture ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:11:32.801754Z","iopub.execute_input":"2025-12-10T23:11:32.802089Z","iopub.status.idle":"2025-12-10T23:11:32.808853Z","shell.execute_reply.started":"2025-12-10T23:11:32.802063Z","shell.execute_reply":"2025-12-10T23:11:32.808290Z"}},"outputs":[{"name":"stdout","text":"✓ Model architecture ready\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================\n# TRAIN BASE MODELS (3-Model Ensemble)\n# This recreates your 0.209 baseline\n# ============================================\n\ndef train_aspect(aspect_char, n_terms=1500, n_models=3):\n    \"\"\"Train ensemble for one GO aspect\"\"\"\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"TRAINING: Aspect {aspect_char}\")\n    print(f\"{'='*50}\")\n    \n    # Get top terms\n    aspect_terms = terms_df[terms_df['aspect'] == aspect_char]\n    top_terms = aspect_terms['term'].value_counts().index[:n_terms].tolist()\n    term_map = {t: i for i, t in enumerate(top_terms)}\n    \n    # Build label matrix\n    label_matrix = np.zeros((len(train_ids), len(top_terms)), dtype=np.float32)\n    relevant = aspect_terms[aspect_terms['term'].isin(top_terms)]\n    \n    for _, row in tqdm(relevant.iterrows(), desc=\"Building labels\", total=len(relevant)):\n        if row['id'] in id_to_idx:\n            label_matrix[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    print(f\"✓ Label matrix: {label_matrix.shape}, sparsity: {(label_matrix > 0).mean():.4f}\")\n    \n    # Train ensemble\n    for model_idx in range(n_models):\n        print(f\"\\n  Model {model_idx+1}/{n_models}:\")\n        torch.manual_seed(42 + model_idx)\n        \n        dataset = SimpleDataset(train_emb, label_matrix)\n        loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)\n        \n        model = SimpleModel(1280, len(top_terms)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        criterion = nn.BCEWithLogitsLoss()\n        \n        for epoch in range(10):\n            model.train()\n            total_loss = 0\n            \n            for X_batch, y_batch in tqdm(loader, desc=f\"    Epoch {epoch+1}/10\", leave=False):\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                \n                optimizer.zero_grad()\n                loss = criterion(model(X_batch), y_batch)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n            \n            if (epoch + 1) % 3 == 0:\n                print(f\"    Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}\")\n        \n        # Save model\n        torch.save(model.state_dict(), f\"base_model_{aspect_char}_{model_idx}.pth\")\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    return top_terms\n\n# Train all aspects\nprint(\"STAGE 1: Training Base Models\")\nprint(\"=\"*60)\n\nresults = {}\nfor char in ['F', 'P', 'C']:\n    results[char] = train_aspect(char)\n\nprint(\"\\n✅ Base models trained and saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:11:52.462700Z","iopub.execute_input":"2025-12-10T23:11:52.463325Z","iopub.status.idle":"2025-12-10T23:14:35.200273Z","shell.execute_reply.started":"2025-12-10T23:11:52.463297Z","shell.execute_reply":"2025-12-10T23:14:35.199561Z"}},"outputs":[{"name":"stdout","text":"STAGE 1: Training Base Models\n============================================================\n\n==================================================\nTRAINING: Aspect F\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Building labels: 100%|██████████| 112061/112061 [00:04<00:00, 24588.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Label matrix: (82404, 1500), sparsity: 0.0009\n\n  Model 1/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n  Model 2/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n  Model 3/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0030\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTRAINING: Aspect P\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Building labels: 100%|██████████| 143554/143554 [00:05<00:00, 24382.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Label matrix: (82404, 1500), sparsity: 0.0012\n\n  Model 1/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n  Model 2/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n  Model 3/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nTRAINING: Aspect C\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Building labels: 100%|██████████| 154977/154977 [00:06<00:00, 24481.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Label matrix: (82404, 1500), sparsity: 0.0013\n\n  Model 1/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0053\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0047\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0044\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n  Model 2/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0053\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0047\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0044\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n  Model 3/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0053\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0047\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0044\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\n✅ Base models trained and saved!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================\n# PSEUDO-LABELING: Generate high-confidence predictions\n# ============================================\n\nprint(\"\\nSTAGE 2: Generating Pseudo-Labels\")\nprint(\"=\"*60)\n\nCONFIDENCE_THRESHOLD = 0.75  # Only use very confident predictions\n\npseudo_labels = {char: {} for char in ['F', 'P', 'C']}  # {aspect: {protein_id: {term: score}}}\n\nfor char in ['F', 'P', 'C']:\n    print(f\"\\nProcessing aspect {char}...\")\n    terms = results[char]\n    \n    # Load models and ensemble predict\n    all_preds = []\n    \n    for model_idx in range(3):\n        model = SimpleModel(1280, len(terms)).to(device)\n        model.load_state_dict(torch.load(f\"base_model_{char}_{model_idx}.pth\"))\n        model.eval()\n        \n        preds = []\n        test_loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024, num_workers=2)\n        \n        with torch.no_grad():\n            for X_batch in tqdm(test_loader, desc=f\"  Model {model_idx+1}\", leave=False):\n                preds.append(torch.sigmoid(model(X_batch.to(device))).cpu().numpy())\n        \n        all_preds.append(np.vstack(preds))\n        del model\n    \n    # Average ensemble\n    final_preds = np.mean(all_preds, axis=0)\n    del all_preds\n    gc.collect()\n    \n    # Extract high-confidence predictions\n    high_conf_count = 0\n    for i, protein_id in enumerate(test_ids):\n        high_conf_indices = np.where(final_preds[i] > CONFIDENCE_THRESHOLD)[0]\n        \n        if len(high_conf_indices) > 0:\n            pseudo_labels[char][protein_id] = {}\n            for idx in high_conf_indices:\n                pseudo_labels[char][protein_id][terms[idx]] = final_preds[i, idx]\n            high_conf_count += len(high_conf_indices)\n    \n    print(f\"  ✓ Generated {high_conf_count:,} pseudo-labels for aspect {char}\")\n\ntotal_pseudo = sum(len(v) for char in pseudo_labels for v in pseudo_labels[char].values())\nprint(f\"\\n✅ Total pseudo-labels: {total_pseudo:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:14:40.090124Z","iopub.execute_input":"2025-12-10T23:14:40.090575Z","iopub.status.idle":"2025-12-10T23:15:19.472041Z","shell.execute_reply.started":"2025-12-10T23:14:40.090546Z","shell.execute_reply":"2025-12-10T23:15:19.471248Z"}},"outputs":[{"name":"stdout","text":"\nSTAGE 2: Generating Pseudo-Labels\n============================================================\n\nProcessing aspect F...\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"  ✓ Generated 5,854 pseudo-labels for aspect F\n\nProcessing aspect P...\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"  ✓ Generated 1,406 pseudo-labels for aspect P\n\nProcessing aspect C...\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"  ✓ Generated 13,073 pseudo-labels for aspect C\n\n✅ Total pseudo-labels: 20,333\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================\n# AUGMENT TRAINING DATA with Pseudo-Labels\n# ============================================\n\nprint(\"\\nSTAGE 3: Creating Augmented Training Set\")\nprint(\"=\"*60)\n\n# Create augmented embeddings and labels\n# We'll add a SUBSET of test proteins with high-confidence predictions\n\nPSEUDO_PROTEINS_PER_ASPECT = 5000  # Limit to avoid overfitting\n\naugmented_data = {}\n\nfor char in ['F', 'P', 'C']:\n    print(f\"\\nAspect {char}:\")\n    terms = results[char]\n    term_map = {t: i for i, t in enumerate(terms)}\n    \n    # Select top pseudo-labeled proteins (most confident)\n    protein_confidences = {}\n    for pid, term_scores in pseudo_labels[char].items():\n        protein_confidences[pid] = np.mean(list(term_scores.values()))\n    \n    # Sort by confidence and take top N\n    sorted_proteins = sorted(protein_confidences.items(), key=lambda x: -x[1])\n    selected_proteins = [pid for pid, conf in sorted_proteins[:PSEUDO_PROTEINS_PER_ASPECT]]\n    \n    print(f\"  Selected {len(selected_proteins):,} pseudo-labeled proteins\")\n    \n    # Get their embeddings\n    test_id_to_idx = {pid: i for i, pid in enumerate(test_ids)}\n    pseudo_indices = [test_id_to_idx[pid] for pid in selected_proteins if pid in test_id_to_idx]\n    pseudo_emb = test_emb[pseudo_indices]\n    \n    # Build pseudo labels\n    pseudo_label_matrix = np.zeros((len(pseudo_indices), len(terms)), dtype=np.float32)\n    for i, pid in enumerate(selected_proteins[:len(pseudo_indices)]):\n        for term, score in pseudo_labels[char][pid].items():\n            if term in term_map:\n                pseudo_label_matrix[i, term_map[term]] = score\n    \n    # Combine with original training data\n    original_labels = np.zeros((len(train_ids), len(terms)), dtype=np.float32)\n    aspect_terms = terms_df[terms_df['aspect'] == char]\n    relevant = aspect_terms[aspect_terms['term'].isin(terms)]\n    \n    for _, row in relevant.iterrows():\n        if row['id'] in id_to_idx and row['term'] in term_map:\n            original_labels[id_to_idx[row['id']], term_map[row['term']]] = 1.0\n    \n    # Stack\n    augmented_emb = np.vstack([train_emb, pseudo_emb])\n    augmented_labels = np.vstack([original_labels, pseudo_label_matrix])\n    \n    augmented_data[char] = {\n        'embeddings': augmented_emb,\n        'labels': augmented_labels,\n        'terms': terms\n    }\n    \n    print(f\"  ✓ Augmented: {augmented_emb.shape[0]:,} total proteins\")\n    print(f\"  ✓ Added {len(pseudo_indices):,} pseudo-labeled samples\")\n\nprint(\"\\n✅ Augmented datasets ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:15:57.255743Z","iopub.execute_input":"2025-12-10T23:15:57.256057Z","iopub.status.idle":"2025-12-10T23:16:16.345940Z","shell.execute_reply.started":"2025-12-10T23:15:57.256031Z","shell.execute_reply":"2025-12-10T23:16:16.345159Z"}},"outputs":[{"name":"stdout","text":"\nSTAGE 3: Creating Augmented Training Set\n============================================================\n\nAspect F:\n  Selected 5,000 pseudo-labeled proteins\n  ✓ Augmented: 87,404 total proteins\n  ✓ Added 5,000 pseudo-labeled samples\n\nAspect P:\n  Selected 1,314 pseudo-labeled proteins\n  ✓ Augmented: 83,718 total proteins\n  ✓ Added 1,314 pseudo-labeled samples\n\nAspect C:\n  Selected 5,000 pseudo-labeled proteins\n  ✓ Augmented: 87,404 total proteins\n  ✓ Added 5,000 pseudo-labeled samples\n\n✅ Augmented datasets ready!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================\n# RETRAIN MODELS on Augmented Data\n# ============================================\n\nprint(\"\\nSTAGE 4: Retraining on Augmented Data\")\nprint(\"=\"*60)\n\nfor char in ['F', 'P', 'C']:\n    print(f\"\\n{'='*50}\")\n    print(f\"RETRAINING: Aspect {char}\")\n    print(f\"{'='*50}\")\n    \n    data = augmented_data[char]\n    X_aug = data['embeddings']\n    y_aug = data['labels']\n    terms = data['terms']\n    \n    # Train new ensemble\n    for model_idx in range(3):\n        print(f\"\\n  Model {model_idx+1}/3:\")\n        torch.manual_seed(100 + model_idx)  # Different seed than base\n        \n        dataset = SimpleDataset(X_aug, y_aug)\n        loader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=2)\n        \n        model = SimpleModel(1280, len(terms)).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=8e-4)  # Slightly lower LR\n        criterion = nn.BCEWithLogitsLoss()\n        \n        for epoch in range(12):  # More epochs\n            model.train()\n            total_loss = 0\n            \n            for X_batch, y_batch in tqdm(loader, desc=f\"    Epoch {epoch+1}/12\", leave=False):\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                \n                optimizer.zero_grad()\n                loss = criterion(model(X_batch), y_batch)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n            \n            if (epoch + 1) % 3 == 0:\n                print(f\"    Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}\")\n        \n        # Save\n        torch.save(model.state_dict(), f\"pseudo_model_{char}_{model_idx}.pth\")\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n\nprint(\"\\n✅ Pseudo-labeled models trained!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:16:24.837606Z","iopub.execute_input":"2025-12-10T23:16:24.838407Z","iopub.status.idle":"2025-12-10T23:19:30.746825Z","shell.execute_reply.started":"2025-12-10T23:16:24.838375Z","shell.execute_reply":"2025-12-10T23:19:30.746095Z"}},"outputs":[{"name":"stdout","text":"\nSTAGE 4: Retraining on Augmented Data\n============================================================\n\n==================================================\nRETRAINING: Aspect F\n==================================================\n\n  Model 1/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0039\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0029\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0027\n\n  Model 2/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0039\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0032\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0029\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0027\n\n  Model 3/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0033\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0029\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0027\n\n==================================================\nRETRAINING: Aspect P\n==================================================\n\n  Model 1/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0059\n\n  Model 2/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0058\n\n  Model 3/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0073\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0066\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0062\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0059\n\n==================================================\nRETRAINING: Aspect C\n==================================================\n\n  Model 1/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0052\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0046\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0043\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0040\n\n  Model 2/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0051\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0046\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0043\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0040\n\n  Model 3/3:\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 3: Loss = 0.0052\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 6: Loss = 0.0046\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 9: Loss = 0.0042\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"    Epoch 12: Loss = 0.0040\n\n✅ Pseudo-labeled models trained!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# GENERATE SUBMISSION with Pseudo-Labeled Models\n# ============================================\n\n!pip install obonet -q\nimport obonet\n\nprint(\"\\nSTAGE 5: Generating Final Submission\")\nprint(\"=\"*60)\n\n# Load GO graph for propagation\ngraph = obonet.read_obo(PATHS[\"go_obo\"])\nparent_map = {n: list(graph.successors(n)) for n in graph.nodes()}\n\n# Prediction parameters\nTHRESHOLD = 0.01\nTOPK_BEFORE_PROP = 80\nTOPK_AFTER_PROP = 80\n\nwith open(\"submission_pseudo.tsv\", \"w\") as f:\n    for char in ['F', 'P', 'C']:\n        print(f\"\\nProcessing aspect {char}...\")\n        terms = results[char]\n        \n        # Ensemble predict with NEW models\n        all_preds = []\n        for model_idx in range(3):\n            model = SimpleModel(1280, len(terms)).to(device)\n            model.load_state_dict(torch.load(f\"pseudo_model_{char}_{model_idx}.pth\"))\n            model.eval()\n            \n            preds = []\n            test_loader = DataLoader(torch.from_numpy(test_emb), batch_size=1024)\n            \n            with torch.no_grad():\n                for X_batch in tqdm(test_loader, desc=f\"  Model {model_idx+1}\", leave=False):\n                    preds.append(torch.sigmoid(model(X_batch.to(device))).cpu().numpy())\n            \n            all_preds.append(np.vstack(preds))\n            del model\n        \n        final_preds = np.mean(all_preds, axis=0)\n        del all_preds\n        gc.collect()\n        \n        # Process each protein\n        for i, protein_id in enumerate(tqdm(test_ids, desc=\"  Writing\", leave=False)):\n            # Filter and top-k\n            scores = {terms[j]: final_preds[i, j] for j in range(len(terms)) if final_preds[i, j] > THRESHOLD}\n            if len(scores) > TOPK_BEFORE_PROP:\n                scores = dict(sorted(scores.items(), key=lambda x: -x[1])[:TOPK_BEFORE_PROP])\n            \n            # Propagate\n            final_scores = scores.copy()\n            queue = list(scores.keys())\n            visited = set(queue)\n            \n            while queue:\n                term = queue.pop(0)\n                score = final_scores.get(term, 0)\n                for parent in parent_map.get(term, []):\n                    if final_scores.get(parent, 0) < score:\n                        final_scores[parent] = score\n                        if parent not in visited:\n                            queue.append(parent)\n                            visited.add(parent)\n            \n            # Write top results\n            for term, score in sorted(final_scores.items(), key=lambda x: -x[1])[:TOPK_AFTER_PROP]:\n                if score > 0.001:\n                    f.write(f\"{protein_id}\\t{term}\\t{score:.3f}\\n\")\n\nprint(\"\\n✅ Submission file created: submission_pseudo.tsv\")\nprint(\"Expected improvement: 0.209 → 0.24-0.26\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T23:19:35.691992Z","iopub.execute_input":"2025-12-10T23:19:35.692741Z","iopub.status.idle":"2025-12-10T23:50:12.770339Z","shell.execute_reply.started":"2025-12-10T23:19:35.692711Z","shell.execute_reply":"2025-12-10T23:50:12.769539Z"}},"outputs":[{"name":"stdout","text":"\nSTAGE 5: Generating Final Submission\n============================================================\n\nProcessing aspect F...\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\nProcessing aspect P...\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"\nProcessing aspect C...\n","output_type":"stream"},{"name":"stderr","text":"                                                                   ","output_type":"stream"},{"name":"stdout","text":"\n✅ Submission file created: submission_pseudo.tsv\nExpected improvement: 0.209 → 0.24-0.26\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}